\chapter[quantitative-rules]{初等采样理论}

至此，我们所拥有并且有效的数学基础由基本的乘法和加法规则构成：

\placeformula[3-1]
\startformula
P(AB|C) = P(A|BC)P(B|C) = P(B|AC)P(A|C)
\stopformula

\placeformula[3-2]
\startformula
P(A|B) + P(\itbar{A}|B) = 1
\stopformula

从它们可以导出扩展的加法规则：

\placeformula[3-3]
\startformula
P(A + B|C) = P(A|C) + P(B|C) - P(AB|C)
\stopformula

使用一致性的祈求 (\Roman{3}c)，亦即无差异性原理：如果一组假设 $(H_1,H_2,\cdots,H_N)$ 在背景信息 $B$ 上相互独立且详尽，并且 $B$ 不会偏好这些假设中的任何一个，那么

\placeformula[3-4]
\startformula
P(H_i|B) = \frac{1}{N}\quad\quad 1\le i \le N
\stopformula

从 (\in[3-3]) 和 (\in[3-4]) 又可以推导出 Bernoulli 瓮规则：如果 $B$ 指定了 $A$ 在 $M$ 个假设所构成的子集上为真，而在剩下的 $(N - M)$ 假设所构成的子集为假，那么

\placeformula[3-5]
\startformula
P(A|B) = \frac{M}{N}
\stopformula

意识到概率论在内容上有多少能够仅仅从此式推导出来，这一点非常重要。

实际上，当前所教授的概率论的全部内容，加上许多经常重要结果——它们经常被视为超出了概率论范围，可以从上述基础推导出来。接下来的几章内容会给出一些细节，而后在第 11 章，继续发展我们的机器人的大脑，让它能够充分理解高级应用所需要的另外一些原理。

在本章中，我们的概率论，它的第一个应用与我们之后所期望达到的严肃科学推断相比，相当简单和幼稚。无论如何，我们从细节上来考虑这些，原因并不仅仅是面向教学。对于这些最为简单的应用，对其逻辑的误解，是几十年来阻碍科学推断发展进程——也包含科学本身——的主要原因之一。因而我们强烈建议读者，即使你早已熟悉初等采样理论，在处理更为复杂的问题之前，也应当认真消化本章内容。

\section[sec-3-1]{无放回采样}

为了让 Bernoulli 瓮更为明确，我们定义了以下命题：

\definedescription[mydefinition][location=left, before=, after=, headstyle=\tf, width=broad, distance=0.25em]
\def\myproposition#1{\hbox to 1.5em{#1}$\equiv$}
\mydefinition{\myproposition{$B$}} 有个瓮，里面有 $N$ 个球。它们的编号是 $(1,2,\cdots,N)$，其中有 $M$ 个球是红色的，其余的是白色的，$0\le M \le N$。除此之外，这些球的各方面都相同。闭着眼从瓮中抽取一个球，观察并记录它的颜色，再把它放回去。重复这一过程 $n$ 次，$0\le n \le N$。\par
\mydefinition{\myproposition{$R_i$}} 第 $i$ 次取的球为红色。\par
\mydefinition{\myproposition{$W_i$}} 第 $i$ 次取的球为白色。\par

根据 $B$，能够取的球仅有红色和白色，有

\placeformula[3-6]
\startformula
P(R_i|B) + P(W_i|B) = 1\quad\quad 1\le i \le N
\stopformula

它等同于说，在知识 $B$ 所创立的\quotation{逻辑环境}里，命题 $R_i$ 与 $W_i$ 的关系是互反的，即

\placeformula[3-7]
\startformula
\itbar{R_i} = W_i\quad\quad \itbar{W_i} = R_i
\stopformula

并且，对于第一次抽取，(\in[3-5]) 变为

\placeformula[3-8]
\startformula
P(R_i|B) = \frac{M}{N}
\stopformula

\placeformula[3-9]
\startformula
P(W_i|B) = 1 - \frac{M}{N}
\stopformula

现在来看这意味着什么。概率赋值 (\in[3-8]) 和 (\in[3-9]) 所断言的并非瓮的物理属性或其内容；它们描述的是机器人在取球之前所具备的{\bf 知识状态}。实际上，倘若机器人的知识状态与上面定义的 $B$ 有所不同（例如，它知道红球和白球在瓮内的位置，或者它不知道 $N$ 和 $M$ 多大），那么它为 $R_i$ 和 $W_i$ 所赋的概率必然与 (\in[3-8]) 和 (\in[3-9]) 不同，而瓮的现实属性却不会变。

因此，称用瓮来做实验以\quotation{验证}(\in[3-8])，这种说法，如同在狗身上做实验来验证一个孩子对他的狗的喜爱，毫无逻辑可言。眼下，我们关心的是来自不完备信息的一致性推理所涉及的逻辑，而不是瓮中取出什么东西这一物理现象的断言（任何情况下，着都是不可能做到的，因为信息 $B$ 不完备）。

最终，我们的机器人将会能够作出一些非常肯定的现实预测。这些预测能够接近，但是（除非在一些退化的情况下）并不能真正达到逻辑演绎的确定性；但是，在我们能够说出什么量能够被准确预测并且为此需要何种信息之前，理论还需要进一步发展。换言之，由机器人在不同知识状态下所赋的概率与实验中可观察到的事实，这二者之间的关系可能并非随意建立的；我们有充分的理由仅仅使用可由概率论规则推导出来的那些关系，这是是我们现在正要去做的的事。

当我们向机器人索取与第二次抽取相关的概率时，机器人的知识状态便会出现变化。例如，开始的两次抽取，取到的皆为红球的概率是多大？根据乘法规则，这个概率为

\placeformula[3-10]
\startformula
P(R_1R_2|B) = P(R_1|B)P(R_2|R_1B)
\stopformula

在最后一个因子中，机器人必须考虑第一所取的红球已经从瓮中移除，因此瓮中所剩下的 $(N - 1)$ 个球中有 $(M - 1)$ 个是红球。因而

\placeformula[3-11]
\startformula
P(R_1R_2|B) = \frac{M}{N}\frac{M - 1}{N - 1}
\stopformula

沿着这一思路，前 $r$ 次连续抽取，所取之球皆为红球的概率为

\placeformula[3-12]
\startformula
\startmathalignment[style=\displaystyle]
\NC P(R_1R_2\cdots R_r|B) \NC = \frac{M(M - 1)\cdots (M - r + 1)}{N(N - 1)\cdots (N - r + 1)}\NR
\NC \NC = \frac{M!(N - r)!}{(M - r)!N!}\quad\quad r\le M\NR
\stopmathalignment
\stopformula

倘若我们用伽马函数关系 $n! = \Gamma(n + 1)$ 来定义阶乘，则限制条件 $r\le M$ 并非必须，因为当 $r > M$ 时，负整数的阶乘结果为无穷大，于是 (\in[3-12]) 的结果自动为 0。

前 $w$ 次抽取，取到的球皆为白球的概率与 (\in[3-12]) 相似，只是要将 $M$ 换成 $(N - M)$：

\placeformula[3-13]
\startformula
P(W_1W_2\cdots W_w|B) = \frac{(N - M)!(N - w)!}{(N - M - w)!N!}
\stopformula

假设前 $r$ 次抽取，取到的皆为红球，那么在 $(r + 1, r + 2, \cdots,r + w)$ 次抽取，取到的皆为白球的概率，需要在 (\in[3-13]) 的基础上考虑 $N$ 和 $M$ 已分别缩减为 $(N - r)$ 和 $(M - r)$：

\placeformula[3-14]
\startformula
P(W_1W_2\cdots W_w|R_1\cdots R_rB) = \frac{(N - M)!(N - r - w)!}{(N - M - w)!(N - r)!}
\stopformula

因此，通过乘法规则，在 $n$ 次抽取过程中，取出 $r$ 个红球之后，又取出 $w = n - r$ 个白球的概率，由 (\in[3-12]) 和 (\in[3-14]) 可得

\placeformula[3-15]
\startformula
P(R_1\cdots R_rW_{r + 1}\cdots W_n|R_1\cdots R_rB) = \frac{M!(N - M)!(N - n)!}{(M - r)!(N - M - w)!N!}
\stopformula

$(N - r)!$ 项被约掉了。

尽管这个结果基于红球和白球特定的取出顺序推导出来，但是以任何特定顺序在 $n$ 次抽取中取出 $r$ 个红球，概率都是相同的。为了看清这一点，需要采用下面的方式

\placeformula[3-16]
\startformula
\frac{M!}{(M - r)!} = M(M - 1)\cdots (M - r + 1)
\stopformula

对 (\in[3-15]) 的其他比率作更完全地展开。于是 (\in[3-15]) 的右部变为

\placeformula[3-17]
\startformula
\frac{M(M - 1)\cdots (M - r + 1)(N - M)(N - M - 1)\cdots (N - M - w + 1)}{N(N - 1)\cdots (N - n + 1)}
\stopformula

现在假设 $r$ 个红球和 $(n - r) = w$ 个白球以任意次序被取出。这种情况的概率是 $n$ 个因子的积；每次取到红球，就会存在一个因子 $\frac{(\text{瓮内红球的数量})}{(\text{瓮内球的总数})}$，并且，同样可以为取到白球写出相似的因子。每次抽取，瓮内的球的数量便会少一个；因而第 $k$ 次抽取，$(N - k + 1)$ 就会在分母中出现，无论在取这个球之前所取的那些球是什么颜色。

就在第 $k$ 个红球被取出之前，无论这个球是在第 $k$ 次还是第 $k$ 次之后的任意一次抽取时被取出，瓮内剩下 $(M - k + 1)$ 个红球；因而，取第 $k$ 个球，就相当于在分子上放了一个因子 $(M - k + 1)$。就在第 $k$ 个白球被取出之前，瓮内剩下 $(N - M - k + 1)$ 个红球；因而，取第 $k$ 个球，无论这个球是在第 $k$ 次还是第 $k$ 次之后的任意一次抽取时被取出，就相当于在分子上放了一个因子 $(N - M - k + 1)$。因而，此时 $n$ 个球皆已取出，其中 $r$ 个是红球，我们在分子和分母上累积的因子与 (\in[3-17]) 相同；不同的抽取次序仅仅是排列分子中因子的次序罢了。因此，在 $n$ 次抽取过程中，以任意次序刚好取出 $r$ 个红球，这种情况的概率皆由 (\in[3-15]) 而定。

务必注意，在这个结果中，乘法规则是以一种特殊的方式展开的，这种方式向我们揭示了如何将计算组织成因子的乘积，{\bf 给定之前的所有抽取结果}，每一个因子都是一次特定的抽取结果的概率。但是还有许多其他方式可以将这个乘法规则展开，每一种展开结果都能够给出以不止所有之前的抽取结果为条件的因子；所有的这些计算一定会得到相同的结果，这是一个事实，一个非常重要的一致性性质，是第 \in[quantitative-rules] 章的推导所要致力确定的。

接下来，我们问：在 $n$ 次抽取过程中，无关乎次序，恰好取到 $r$ 个红球的概率是多大？红球与白球出现不同的次序的可能性相互独立，因此我们要把这些可能性叠加起来；由于每一项都等于 (\in[3-15])，所以只需要用二项式系数

\placeformula[3-18]
\startformula
\startpmatrix n\NR r\NR\stoppmatrix = \frac{n!}{r!(n - r)!}
\stopformula

乘以 (\in[3-15])。二项式系数表示 $n$ 次抽取恰好取到 $r$ 个红球的可能取法的数量，我们将其称为事件 $r$ 的{\bf 多样性}。例如，在三次抽取过程中取到三个红球，只有一种取法，因为

\placeformula[3-19]
\startformula
\startpmatrix 3\NR 3\NR\stoppmatrix = 1
\stopformula

这种取法是 $R_1R_2R_3$，亦即事件 $r = 3$ 的多样性为 1。但是，要从三次抽取过程中获得两个红球，取法有三种，因为

\placeformula[3-20]
\startformula
\startpmatrix 3\NR 2\NR\stoppmatrix = 3
\stopformula

这三种取法分别是 $R_1R_2W_3$、$R_1W_2R_3$ 以及 $W_1R_2R_3$，因此事件 $r = 2$ 的多样性为 $3$。

\startExercise
为什么 (\in[3-18]) 的多样性因子不是 $n!$？毕竟，我们是从规定小球不仅有颜色而且也有标签 $(1,2,\cdots,N)$，因此红球本身的不同排列——为 (\in[3-18]) 的分母贡献了 $r!$，造成有区别的 $n$ 次抽取结果。

提示：在 (\in[3-15]) 中，我们没有指定哪些红球和哪些白球要被抽取。
\stopExercise

计算 (\in[3-15]) 和 (\in[3-18]) 的积，许多阶乘可以重新组织成三个二项式系数。定义 $A\equiv$\quotation{以任意次序取 $n$ 个球，其中有 $r$ 个红球}，还有函数

\placeformula[3-21]
\startformula
h(r|N, M, n) \equiv P(A|B)
\stopformula

这样就有

\placeformula[3-22]
\startformula
h(r|N, M, n) = \frac{\startpmatrix M\NR r\NR\stoppmatrix\startpmatrix N - M\NR n - r\NR\stoppmatrix}{\startpmatrix N\NR n\NR\stoppmatrix}
\stopformula

我们应当将其缩写为 $h(r)$。通过约定 $x! = \Tau(x + 1)$，当 $r > M$ 或 $r > n$ 或者 $(n - r) > (N - M)$ 时，$h(r)$ 就会自动变为 $0$，理应如此。

在此，我们动用了记法上的小伎俩，原因在附录 B 部分给出了解释。要点是，正式的概率符号 $P(A|B)$，用的是大写字母 $P$，参数 $A$ 和 $B$ 总是表示命题，它们可以是相当复杂的语言陈述。若想用普通数字作为参数，那么为了一致性，我们应当定义新的函数符号，诸如 $h(r|N, M, n)$。坚持使用 $P(r|NMn)$，忽视了 $A$ 和 $B$ 定性方面的约定，会因为曲解了这些方程的意思而导致严重的错误（例如以后要讨论的一些生僻的悖论）。不过，如第 \in[quantitative-rules] 章所述，我们通过使用 $p(A|B)$ 或 $p(r|n)$ 这样的使用小写字母 $p$ 的概率符号来迎合现代的工作，在这些符号中，我们允许使用命题或代数变量作为参数；在这种情况下，它们的含义由语境而定。

基本结果 (\in[3-22]) 被称为\quotation{超几何分布}，因为它与高斯超几何函数的幂级数形式

\placeformula[3-23]
\startformula
F(a,b,c;t) = \sum_{r = 0}^{\infty}\frac{\Tau(a + r)\Tau(b + r)\Tau(c)}{\Tau(a)\Tau(b)\Tau(c + r)}\frac{t^r}{r!}
\stopformula

的系数有关。如果 $a$ 和 $b$ 中有一个为负整数，这个级数会收敛，因此它是个多项式。很容易验证，{\bf 生成函数}

\placeformula[3-24]
\startformula
G(t) \equiv \sum_{r = 0}^nh(r|N, M, n)t^r
\stopformula

等于

\placeformula[3-25]
\startformula
G(t) = \frac{F(-M,-n,c;t)}{F(-M,-n,c;1)}
\stopformula

其中 $c = N - M - n + 1$。显然 $G(1) = 1$，根据 (\in[3-24])，这个等式描述的是超几何分布被正确地归一化了。在 (\in[3-25]) 的结果中，$G(t)$ 满足二阶超几何微分方程，并且有一些与计算有用的其他性质。

尽管超几何分布 $h(r)$ 看上去很复杂，但是它有着一些令人惊讶的简单性质。通过设定 $h(r') = h(r' - 1)$，$r$ 最有可能的值被发现位于一个单位之内。我们发现的是

\placeformula[3-26]
\startformula
r' = \frac{(n + 1)(M + 1)}{N + 2}
\stopformula

如果 $r'$ 是整数，那么 $r'$ 和 $r' - 1$ 都是最有可能的值。如果 $r'$ 并非整数，那么就会有唯一的最可能的值是

\placeformula[3-27]
\startformula
\hat{r}=\text{INT}(r')
\stopformula

即比 $r'$ 小的整型值。因而采样抽取中红球的最有可能的分数 $f = \frac{r}{m}$，如同我们的直觉所期盼的那样，它近似等于分数 $F = \frac{M}{N}$，后者是瓮内原有的红球的个数与球的总数的比值。在现实预测方面，这是我们的第一个粗略的示例：我们的信息所指定的量 $F$ 与现实实验所测定的量 $f$ 之间的关系，而这个关系是根据概率论推导出来的。

超几何分布 $h(r)$ 的宽度给出了精确性指标，利用这个指标，机器人可以判定 $r$。许多这样的问题，它们的答案可通过计算{\bf 累加概率}而得到，即寻找 $R$ 个或更少的红球的概率。如果 $R$ 是整数，累加概率就是

\placeformula[3-28]
\startformula
H(R) \equiv\sum_{r = 0}^Rh(r)
\stopformula

不过，为了后文中给出正式的原因，我们将 $H(x)$ 定义成一种阶梯函数，$x$ 值为所有非负实数；因而 $H(x)=H(R)$，其中 $R = \text{INT}(x)$ 是不大于 $x$ 的最大整数。

对于诸如 $h(r)$ 这样的概率分布，将其{\bf 中值}定义为一个数 $m$，命题 $r < m$ 和 $r > m$ 被赋予相等的概率。严格而言，根据这个定义，一个离散的分布通常没有中值。倘若存在一个整数 $R$，满足 $H(R - 1) = 1 - H(R)$，并且 $H(R) > H(R - 1)$，则 $R$ 是唯一的中值。倘若有一个整数 $R$，满足 $H(R) = \frac{1}{2}$，则 $R\le r < R'$ 之内的任意 $r$ 都是中值，其中 $R'$ 是 $H(x)$ 的下一个比较高的跃点；否则不存在中值。

但是对于大多数目的而言，我们可能要宽容一些，以接近定义为准。如果 $n$ 是足够大，那么便有理由认为能够让 $H(R)$ 非常接近 $\frac{1}{2}$ 的 $R$ 为\quotation{中值}。以相同的宽容精神，使得 $H(R)$ 非常接近 $\frac{1}{4}$、$\frac{3}{4}$ 的 $R$ 值，可将其称为\quotation{低四等分}和\quotation{高四等分}。如果 $n \gg 10$，使得 $H(R)$ 非常接近 $\frac{k}{10}$ 的 $R$ 值，我们称为\quotation{第 $k$ 个十等分}，以此类推。当 $n \rightarrow \infty$，这些宽松的定义就会变得与严格的定义一致。

通常，$H(R)$ 的细节是不重要的，就我们的目的而言，知道中值和四等分就足够了。这样，在机器人的预测及其准确性方面，$(\text{中值})\pm (\text{四分位距离})$ 提供了一个好的思路\footnote{译注：有误，应该是区间 $[\text{低四等分},\,\text{高四等分}]$。}。亦即，在提供给机器人的信息上，$r$ 的真值落入这个区间之内的可能性与落在该区间之外是相同的。同样地，$r$ 落入第一个到第五个六等分区间 机器人给出的概率值 $(\frac{5}{6}) - \frac{1}{6} = \frac{2}{3}$（换言之，$2:1$ 的几率）；$r$ 落入第一个十等分到第九个十等分的几率是 $8:2 = 4:1$，以此类推。

对于这些我们在过去经常用到的这些分布，尽管可以发展出相当混乱的近似公式，但是现在很容易通过计算机算出精确的分布。例如，W. H. Press 等 (1986) 给出了两个程序，在 $a$、$b$ 和 $c$ 值任意的情况下，它们可以算出广义复超几何分布。表 \in[t-3-1] 与 \in[t-3-2] 分别给出了 $N = 100$、$M = 50$、$n = 10$ 和 $N = 100$、$M = 10$、$n = 50$ 这两种情况下的超几何分布。在后者中，不可能取出多于 $10$ 个球，因此 $r > 10$ 的条目都是 $h(r) = 0$，$H(r) = 1$，因此在表中对它们未作统计。惊人的事实是，$h(r)$ 正值对应的条目是相同的，亦即超几何分布在交换 $M$ 和 $n$ 的情况下具有对称性质

\placeformula[3-29]
\startformula
h(r|N, M, n) = h(r|N, n, M)
\stopformula

无论我们是从包含 50 个红球的瓮中取十个球，还是从包含十个红球的瓮中取 50 个球，在样本抽取过程中取到 $r$ 个红球的概率是相同的。这个性质很容易通过详细检验 (\in[3-22]) 而得以验证；从超几何函数 (\in[3-23]) 的 $a$ 与 $b$ 的对称性来看，这一点也显而易见。

表 \in[t-3-1] 与 \in[t-3-2] 还有着另外一个显而易见的对称性，关于分布尖峰的对称性：$h(r|100, 50， 10) = h(10 - r|100, 50, 10)$。不过，通常情况下并非如此。将 $N$ 的值改为 $99$，就会导致轻微地非对称尖峰，如表 \in[t-3-3] 所示。表 \in[t-3-1] 的尖峰的出现，原因在于：如果交换 $M$ 和 $(N - M)$，并且同时交换 $r$ 和 $(n - r)$，那么就相当于\quotation{红球}和\quotation{白球}这两个单词的交换，因此分布未变：

\placeformula[3-30]
\startformula
h(n - r|N, N - M, n) = h(r|N, M, n)
\stopformula

但是当 $M = \frac{N}{2}$，上式可约化为对称形式

\placeformula[3-31]
\startformula
h(n - r|N, M, n) = h(r|N, M, n)
\stopformula

这正是在表 \in[t-3-1] 中我们所看到的尖峰的对称性。根据 (\in[3-29])，当 $n = \frac{N}{2}$ 时，尖峰也是对称的。

\placetable[here,force][t-3-1]{超几何分布：$N,M,n=100,10,50$}
{\startxtable[offset=0pt]
  \setupinterlinespace[line=1.2em]
  \startxtablehead[topframe=on, rulethickness=1.5pt]
  \startxrow
  \startxcell[width=3cm] $r$ \stopxcell
  \startxcell[width=3cm] $h(r)$ \stopxcell
  \startxcell[width=3cm] $H(r)$ \stopxcell
  \stopxrow
  \stopxtablehead
  \startxtablebody[height=fit]
  \startxrow[topframe=on,rulethickness=0.75pt]
  \startxcell 0 \stopxcell
  \startxcell 0.000593  \stopxcell
  \startxcell 0.000593 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 1 \stopxcell
  \startxcell 0.007237 \stopxcell
  \startxcell 0.007830 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 2 \stopxcell
  \startxcell 0.037993 \stopxcell
  \startxcell 0.045824 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 3 \stopxcell
  \startxcell 0.113096 \stopxcell
  \startxcell 0.158920 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 4 \stopxcell
  \startxcell 0.211413 \stopxcell
  \startxcell 0.370333 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 5 \stopxcell
  \startxcell 0.259334 \stopxcell
  \startxcell 0.629667 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 6 \stopxcell
  \startxcell 0.211413 \stopxcell
  \startxcell 0.841080 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 7 \stopxcell
  \startxcell 0.113096 \stopxcell
  \startxcell 0.954177 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 8 \stopxcell
  \startxcell 0.037993 \stopxcell
  \startxcell 0.992170 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 9 \stopxcell
  \startxcell 0.007237 \stopxcell
  \startxcell 0.999407 \stopxcell
  \stopxrow
  \stopxtablebody
  \startxtablefoot[bottomframe=on,rulethickness=1.5pt]
  \startxrow
  \startxcell 10 \stopxcell
  \startxcell 0.000593 \stopxcell
  \startxcell 1.000000 \stopxcell
  \stopxrow
  \stopxtablefoot
  \stopxtable
}

\placetable[here,force][t-3-2]{超几何分布：$N,M,n=100,50,10$}
{\startxtable[offset=0pt]
  \setupinterlinespace[line=1.2em]
  \startxtablehead[topframe=on, rulethickness=1.5pt]
  \startxrow
  \startxcell[width=3cm] $r$ \stopxcell
  \startxcell[width=3cm] $h(r)$ \stopxcell
  \startxcell[width=3cm] $H(r)$ \stopxcell
  \stopxrow
  \stopxtablehead
  \startxtablebody[height=fit]
  \startxrow[topframe=on,rulethickness=0.75pt]
  \startxcell 0 \stopxcell
  \startxcell 0.000593  \stopxcell
  \startxcell 0.000593 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 1 \stopxcell
  \startxcell 0.007237 \stopxcell
  \startxcell 0.007830 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 2 \stopxcell
  \startxcell 0.037993 \stopxcell
  \startxcell 0.045824 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 3 \stopxcell
  \startxcell 0.113096 \stopxcell
  \startxcell 0.158920 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 4 \stopxcell
  \startxcell 0.211413 \stopxcell
  \startxcell 0.370333 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 5 \stopxcell
  \startxcell 0.259334 \stopxcell
  \startxcell 0.629667 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 6 \stopxcell
  \startxcell 0.211413 \stopxcell
  \startxcell 0.841080 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 7 \stopxcell
  \startxcell 0.113096 \stopxcell
  \startxcell 0.954177 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 8 \stopxcell
  \startxcell 0.037993 \stopxcell
  \startxcell 0.992170 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 9 \stopxcell
  \startxcell 0.007237 \stopxcell
  \startxcell 0.999407 \stopxcell
  \stopxrow
  \stopxtablebody
  \startxtablefoot[bottomframe=on,rulethickness=1.5pt]
  \startxrow
  \startxcell 10 \stopxcell
  \startxcell 0.000593 \stopxcell
  \startxcell 1.000000 \stopxcell
  \stopxrow
  \stopxtablefoot
  \stopxtable
}

\placetable[here,force][t-3-3]{超几何分布：$N,M,n=100,50,10$}
{\startxtable[offset=0pt]
  \setupinterlinespace[line=1.2em]
  \startxtablehead[topframe=on, rulethickness=1.5pt]
  \startxrow
  \startxcell[width=3cm] $r$ \stopxcell
  \startxcell[width=3cm] $h(r)$ \stopxcell
  \startxcell[width=3cm] $H(r)$ \stopxcell
  \stopxrow
  \stopxtablehead
  \startxtablebody[height=fit]
  \startxrow[topframe=on,rulethickness=0.75pt]
  \startxcell 0 \stopxcell
  \startxcell 0.000527 \stopxcell
  \startxcell 0.000527 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 1 \stopxcell
  \startxcell 0.006594 \stopxcell
  \startxcell 0.007121 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 2 \stopxcell
  \startxcell 0.035460 \stopxcell
  \startxcell 0.042581 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 3 \stopxcell
  \startxcell 0.108070 \stopxcell
  \startxcell 0.150651 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 4 \stopxcell
  \startxcell 0.206715 \stopxcell
  \startxcell 0.357367 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 5 \stopxcell
  \startxcell 0.259334 \stopxcell
  \startxcell 0.616700 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 6 \stopxcell
  \startxcell 0.216111 \stopxcell
  \startxcell 0.832812 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 7 \stopxcell
  \startxcell 0.118123 \stopxcell
  \startxcell 0.950934 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 8 \stopxcell
  \startxcell 0.040526 \stopxcell
  \startxcell 0.991461 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 9 \stopxcell
  \startxcell 0.007880 \stopxcell
  \startxcell 0.999341 \stopxcell
  \stopxrow
  \stopxtablebody
  \startxtablefoot[bottomframe=on,rulethickness=1.5pt]
  \startxrow
  \startxcell 10 \stopxcell
  \startxcell 0.000659 \stopxcell
  \startxcell 1.000000 \stopxcell
  \stopxrow
  \stopxtablefoot
  \stopxtable
}

超几何分布还有两个不直观甚至在 (\in[3-22]) 中也难以发现的对称性质。我们问机器人索求第二次取到红球的概率 $P(R_2|B)$。这与 (\in[3-8]) 的计算并不相同，因为机器人知道，在进行第二次抽取之前，瓮内只有 $(N - 1)$ 个球，而非 $N$ 个。但是，机器人不知道第一次抽取而导致瓮内少了的那个球的颜色，因此它不知道瓮内剩下的红球的数量是 $M$ 还是 $(M - 1)$。这样一来，Bernoulli 瓮的的根基 (\in[3-5]) 就丢了，从而导致问题似乎变得不可判定起来。

然而，这个问题终究是可判定的；下面便是概率计算中有用的技巧的第一个示例，其原理是将一个命题分解为几个更为简单的命题的析取，这个原理在第 1 章和第 2 章讨论过了。机器人知道 $R_1$ 和 $W_1$ 必有一个为真，因而基于布尔代数，我们就有

\placeformula[3-32]
\startformula
R_2 = (R_1 + W_1)R_2 = R_1R_2 + W_1R_2
\stopformula

应用加法规则和乘法规则，可得

\placeformula[3-33]
\startformula
\startmathalignment[style=\displaystyle]
\NC P(R_2|B)\NC = P(R_1R_2|B) + P(W_1R_2|B)\NR
\NC \NC = P(R_2|R_1B)P(R_1|B) + P(R_2|W_1B)P(W_1|B)\NR
\stopmathalignment
\stopformula

然而

\placeformula[3-34]
\startformula
P(R_2|R_1B) = \frac{M - 1}{N - 1}\quad\quad P(R_2|W_1B) = \frac{M}{N - 1}
\stopformula

因此

\placeformula[3-35]
\startformula
P(R_2|B) = \frac{M - 1}{N - 1}\frac{M}{N} + \frac{M}{N - 1}\frac{N - M}{N} = \frac{M}{N}
\stopformula

复杂性顷刻之间便消失了，而且第一次和第二次取到红球的概率是相同的。下面继续看第三次会发生什么

\placeformula[3-36]
\startformula
R_3 = (R_1 + W_1)(R_2 + W_2)R_3 = R_1R_2R_3 + R_1W_2R_3 + W_1R_2R_3 + W_1W_2R_3
\stopformula

因此

\placeformula[3-37]
\startformula
\startmathalignment[style=\displaystyle]
\NC P(R_3|B)\NC = \frac{M}{N}\frac{M - 1}{N - 1}\frac{M - 2}{N - 2} + \frac{M}{N}\frac{N - M}{N - 1}\frac{M - 1}{N - 2}\NR
\NC \NC +  \frac{N - M}{N}\frac{M}{N - 1}\frac{M - 1}{N - 2} + \frac{N - M}{N}\frac{N - M - 1}{N - 1}\frac{M}{N - 2}\NR
\NC \NC = \frac{M}{N}
\stopmathalignment
\stopformula

复杂性又一次被我们消除了。对于任意一次取到红球，机器人{\bf 如果不知道任何其他次抽取结果}，它总会赋以与 Bernoulli 瓮的结果 (\in[3-5]) 相同的概率。这就是第一个不明显的对称性。在此，我们不对这个结论的一般性给出证明，因为这个结论是一个更具一般性的结果的特殊情况；见后文的方程 (\in[3-118])。

(\in[3-32]) 和 (\in[3-26]) 所展示的计算方法总结如下：对于需要赋以概率的量，将其分解为相互独立的子命题，然后运用加法规则和乘法规则。若子命题经过合理挑选（亦即，如果他们在问题的语境中有着简单的含义），那么它们的概率通常易于计算。若挑选不当（如同第 2 章末尾所给出的企鹅的例子），则与之相关的计算过程自然是毫无帮助。

\section{逻辑 vs 趋向性}

\in[sec-3-1] 节的结果给我们带来了新的问题。在索求第 $k$ 次抽取结果为红球的概率时，在之前的抽取过程中所取到了何种颜色的球，这一知识显然与我们要解决的问题有关系，因为之前的抽取结果会影响在第 $k$ 次抽取时瓮内红球的数量 $M_k$。那么之后抽取结果会不会也对第 $k$ 次有影响呢？第一感觉是没有影响，因为之后的抽取结果不会影响到 $M_k$ 的值。例如，广为人知的统计力学的阐述（Penrose，1979）依赖一个基本公理，现在发生的事情仅依赖之前发生的事，并不依赖之后发生的事。Penrose 认为这是\quotation{因果}所必须的客观条件。

因而，如同在第 1 章所作的那样，我们需要再一次强调，推断关心的是{\bf 逻辑}联系，它可能与具有赢过关系的物理影响有关，也可能无关。为了表明为什么之后发生的事件的知识与之前发生的事件的概率存在联系，不妨考虑一个已知（背景信息 $B$）仅包含一个红球和一个白球的瓮：$N = 2$，$M = 1$。仅给定这个信息，第一次取到红球的概率为 $P(R_1|B) = \frac{1}{2}$。但是，倘若机器人知道第二次抽到了红球，那么第一次便不可能取到红球，即

\placeformula[3-38]
\startformula
P(R_1|R_2B) = 0
\stopformula

更为一般的是，乘法规则给出

\placeformula[3-39]
\startformula
P(R_jR_k|B) = P(R_j|R_kB)P(R_k|B) = P(R_k|R_jB)P(R_j|B)
\stopformula

但是我们已经看到了，对于所有的 $j$、$k$，$P(R_j|B) = P(R_k|B) = \frac{M}{N}$，因此

\placeformula[3-40]
\startformula
P(R_j|R_kB) = P(R_k|R_jB)
\stopformula

概率论告诉我们的是，后续的抽取结果与第 $k$ 次抽取结果的相关性与之前的抽取结果完全相同！尽管完成后续的某次抽取并不会对第 $k$ 次抽取时瓮内的红球数量 $M_k$ 产生物理影响，但是与下一次抽取有关的{\bf 信息}与之前的某次抽取相比，在我们的在第 $k$ 次抽取时的{\bf 知识状态}上有着同样的影响。这就是第二个不明显的对称性。

这个结果可能会让一些关心\quotation{概率的含义}的一些派系非常不安。尽管逻辑蕴含与物理因果并不相同这一点广为人知，但是尝试将概率 $P(A|B)$ 解释为 $B$ 对 $A$ 的某种程度上的因果影响，这种倾向依然非常顽固。不仅上述的 Penrose 的工作暴露了这种倾向，哲学家 Karl Popper\footnote{在第九次 Colston 研讨会上，Popper (1957) 将他的趋向解释描述为\quotation{纯客观的}，只是回避了\quotation{物理影响}这种描述。实际上，他会认为，抛掷一个骰子，出现哪一面的概率并非骰子的物理形式（如同 Cram\'er (1946) 所坚持的那样），而是整个实验装置的客观性质。当然，{\bf 实验结果}依赖于完整的装置与实验过程，这不过是老生常谈。Niels Bohr 在阐述量子理论时便重复强调这一观点，但是自从伽利略以来，大概没有科学家反对这个观点。然而，除非 Popper 真正的意思是\quotation{物理影响}，否则他的解释就是超自然的，并非客观的。在他的后续论文里 (Popper, 1959)，他对趋向性解释给出了更完备的定义；甚至在单次试验的情况里，一种趋向性也会被认为是\quotation{客观的}和\quotation{物理实在的}。在后文中，我们通过数学论证就会看到一些趋向性解释造成的逻辑上的困境。Popper 抱怨在量子理论里有人\quotation{……纯{\bf 客观}的统计解释与出于我们不完备的知识这种的主观解释}之间摇摆不定，认为后者应当受谴责，并且趋向性解释可以完全抹杀它的存在。他不可能再错更多。在第 9 章中，我们会在概念层次上详细回答这个问题；显然，{\bf 不完备的知识是科学家仅有的可用的原料！}在第 10 章里，我们会考虑硬币抛掷机的详细的物理学原理，并且可以看到抛掷方法如何直接通过物理影响对结果产生影响。} 将概率解释为\quotation{趋向性}则更明显地暴露了这一倾向。

在我们看来，尽管 (\in[3-38]) 这种简单的例子用趋向性来解释，并不会有什么问题，但是诸如 (\in[3-40]) 这样的关系就是趋向性观点无法解释的了。无论怎样，我们所发展的逻辑推断理论，无论是外在还是结果，都与 Penrose 和 Popper 所面对的物理因果理论有着根本区别。显然，逻辑推断可以用在许多问题上，而这些问题会让物理因果的假设失去意义。

这并不意味着我们不能引入\quotation{趋向性}或者物理因果的概念；关键在于无论趋向性是否存在，逻辑推断皆可用且有用。倘若这样的概念（亦即某种趋向性存在）被形式化表示为合理定义的假设，那么我们的概率论可以分析它的蕴含。接下来在 \in[sec-3-10] 节便会这样做。此外，如同我们检验任何一个合理定义的假设那样，我们也能根据证据检验那个在候选假设中最突出的那个。实际上，概率论最常见也是最重要的一项应用便是决断是否存在因果影响的证据：新药是否有效，或者新的工程设计是否可靠？新的制止犯罪的法律是否降低了犯罪率？我们在第 4 章开始研究假设检验。

在所有的科学中，逻辑推断的应用范围相当广泛。我们不反对物理影响可以仅在时间上向前传播，但是我们认为逻辑推断在两个方向上都能同样传播。一个考古学家发掘了一个手工艺品，这件事会改变他对几千年前的事件的认识；如若不然，考古学、地质学以及古生物学就会失去意义。夏洛克·福尔摩斯的推理也是直接由证据出发进行推断过去发生了什么事情。从距离你 600 米的仪仗队传来的声音会改变你对这个仪仗队在两分钟之前所演奏的音乐的认识。听一张 Toscanini 的贝多芬交响乐唱片，会改变你对 Toscanini 从许多年前的他的管弦乐队拮取的声音的认识。

这暗示着，并且我们以后也会予以验证的是，非平衡现象的完备理论，诸如声音传播，需要认识并使用向后的逻辑推断，尽管它们没有表现出物理因果。关键在于我们关于任何现象所能给出的最好的推断——无论是在物理、生物、经济以及任何其他领域——必须考虑我们所具有的所有相关的信息，无论这些信息在实践上比现象本身早或晚；这应该被视为老生常谈，而不应该视为悖论。在本章的最后（习题 \in[ex-3-6]），读者有机会通过计算一个考虑了前向因果影响的后向推断直接论证这一点。

更为一般地，考虑一个概率分布 $p(x_1\cdots x_n|B)$，其中 $x_i$ 表示第 $i$ 次试验的结果，对应的值不止两个（红或白），假设它的可能取值为 $x_i = (1,2,\cdots, k)$，表示 $k$ 种不同的颜色。不管 $x_i$ 如何排列，概率保持不变，那么这个概率就仅依赖于采样数量 $(n_1,\cdots,n_k)$，即 $x_i = 1$ 出现了多少次，$x_i = 2$ 出现了多少次，等等。像这样的分布被称为{\bf 可交换的}；以后我们会发现，可交换分布有许多有趣的数学性质和重要应用。

再回到 Bernoulli 瓮上来，显然，超几何分布是可交换的，因为无论是时间顺序，还是在抽取次序上的顺序，每次抽取都与其他次抽取有着相同的相关性。不过，不止超几何分布如此，对于任何一种可交换的分布（亦即，无论何时，事件序列的概率独立于它们的顺序）皆如此。因此，略加思考，那些对称性，从物理因果的角度是相当难以解释，但是将一切作为逻辑命题，便显而易见。

下面以量化的形式计算一下这个效果。假设 $j < k$，命题 $R_jR_k$（第 $j$ 次和第 $k$ 次取到红球）在布尔代数中可写为

\placeformula[3-41]
\startformula
R_jR_k = (R_1 + W_1)\cdots(R_{j - 1} + W_{j - 1})R_j(R_{j + 1} + W_{j + 1})\cdots(R_{k - 1} + W_{k - 1})R_k
\stopformula

以 (\in[3-36]) 的方式将上式展开为

\placeformula[3-42]
\startformula
2^{j - 1}\times 2^{k - j - 1} = 2^{k - 2}
\stopformula

个命题的逻辑和。其中每个命题形如

\placeformula[3-43]
\startformula
W_1R_2W_3\cdots R_j\cdots R_k
\stopformula

的 $k$ 个结果。概率 $P(R_jR_k|B)$ 是这些命题的概率之和。不过，我们知道，给定 $B$，任何一个序列的概率都独立于红球和白球出现的顺序。因而，我们可以对每个序列进行变更，将 $R_j$ 移到第一个位置，将 $R_k$ 移到第二个位置。这样，我们可以将 $(W_1\cdots R_j\cdots)$ 替换为 $(R_1\cdots W_j\cdots)$，以此类推。对它们重新组合，$(R_1R_2)$ 的后面可以是 $(3,4,\cdots,k)$ 次抽取结果。换言之，$R_jR_k$ 的概率与

\placeformula[3-44]
\startformula
R_1R_2(R_3 + W_3)\cdots(R_k + W_k) = R_1R_2
\stopformula

的概率相等。于是就有

\placeformula[3-45]
\startformula
P(R_jR_k|B) = P(R_1R_2|B) = \frac{M(M - 1)}{N(N - 1)}
\stopformula

类似地有

\placeformula[3-46]
\startformula
P(W_jR_k|B) = P(W_1R_2|B) = \frac{(N - M)M}{N(N - 1)}
\stopformula

因此，根据乘法规则，对于 $j < k$ 有

\placeformula[3-47]
\startformula
P(R_k|R_jB) = \frac{P(R_jR_k|B}{P(R_j|B} = \frac{M - 1}{N - 1}
\stopformula

和

\placeformula[3-48]
\startformula
P(R_k|W_jB) = \frac{P(W_jR_k|B}{P(W_j|B} = \frac{M}{N - 1}
\stopformula

根据 (\in[3-40])，结果 (\in[3-47]) 和 (\in[3-48]) 对于所有 $j\ne k$ 成立。

正如我们所言，这个结论会让许多人吃惊，因此我们要通过一种不同的说法再解释一遍，论述一下要点。机器人知道瓮内一开始有 $M$ 个红球和 $(N - M)$ 个白球。然后，一旦获知前一次取出了一个红球，它就知道了对于后续的抽取而言，红球少了一个。于是，问题就变为，瓮中含有 $(N - 1)$ 个球，其中有 $(M - 1)$ 个红球，从这样的瓮中取球；这样，(\in[3-47]) 不过是 (\in[3-37]) 在不同问题中的解。

但是，知道后续某次抽取结果，为什么同样有效？这是因为机器人知道红球在后续某次被取出，那么必须将一个红球\quotation{搁置}出去方能使得这一效果成为可能。这一信息所带来的结果是，之前可取的红球，其数量需要减一。上例 (\in[3-38]) 是这种情况的特例，其结论相当清楚。

\section{基于非精确信息的推理}

现在我们尝试将这一认识应用于更复杂的问题。假设机器人知道后续抽取过程中至少有一次会取到红球，但是却不知道是哪一次。亦即，这个新的信息，可以表示为布尔代数的命题

\placeformula[3-49]
\startformula
R_{\text{later}} \equiv R_{k + 1} + R_{k+2} + \cdots + R_n
\stopformula

这个信息至少将第 $k$ 次抽取时红球有效的数量减去了一个，但是我们并不清楚 $R_{\text{later}}$ 是否有着 $R_n$ 那样的确切的蕴含。为了弄清楚这一点，我们再一次动用乘法规则的对称性：

\placeformula[3-50]
\startformula
P(R_kR_{\text{later}}|B) = P(R_k|R_{\text{later}}B)P(R_{\text{later}}|B) = P(R_{\text{later}}|R_kB)P(R_k|B)
\stopformula

由它可得

\placeformula[3-51]
\startformula
P(R_k|R_{\text{later}}B) = P(R_k|B)\frac{P(R_{\text{later}}|R_kB)}{P(R_{\text{later}}|B)}
\stopformula

右侧所有的量都很容易计算。

观察 (\in[3-49])，有人可能会像下面这样推理，


\placeformula[3-52]
\startformula
P(R_{\text{later}}|B) = \sum_{j = k + 1}^nP(R_j|B)
\stopformula

这是错误的，原因是，除非 $M = 1$，事件 $R_j$ 并非相互独立。再者，由 (\in[2-82]) 可知，还有许多项有待增补。即便如此，这种计算方法也会非常冗长乏味。

为了便于计算，要注意 $R_{\text{later}}$ 的反命题为后续的抽取过程取到的都是白球

\placeformula[3-53]
\startformula
\itbar{R}_{\text{later}} = W_{k + 1}W_{k + 2}\cdots W_n
\stopformula

因此 $P(\itbar{R}_{\text{later}}|B)$ 是后续取到的皆为白球的概率，这与之前的抽取结果无关（亦即，此时的机器人并不知道之前的抽取结果）。根据可交换性，这个概率与一开始 $(n - k)$ 次连续的抽取结果皆为白球的概率相同，后者也与之后的抽取结果无关；根据 (\in[3-13])，

\placeformula[3-54]
\startformula
P(\itbar{R}_{\text{later}}|B) = \frac{(N - M)!(N - n + k)!}{N!(N - M - n + k)!} = \startpmatrix N - M\NR n - k\NR\stoppmatrix\startpmatrix N \NR n - k\NR\stoppmatrix^{-1}
\stopformula

同样，$P(\itbar{R}_{\text{later}}|R_kB)$ 应当与 $(N - 1)$ 个球并且其中有 $(M - 1)$ 个为红球的情况的概率相同：

\placeformula[3-55]
\startformula
P(\itbar{R}_{\text{later}}|R_kB) = \frac{(N - M)!(N - n + k - 1)!}{(N - 1)!(N - M - n + k)!} = \startpmatrix N - M\NR n - k\NR\stoppmatrix\startpmatrix N - 1 \NR n - k\NR\stoppmatrix^{-1}
\stopformula

现在 (\in[3-51]) 变为

\placeformula[3-56]
\startformula
P(R_k|R_{\text{later}}B) = \frac{M}{N - n + k}\times\frac{\startpmatrix N - 1\NR n - k\NR\stoppmatrix - \startpmatrix N - M\NR n - k\NR\stoppmatrix}{\startpmatrix N\NR n - k\NR\stoppmatrix - \startpmatrix N - M\NR n - k\NR\stoppmatrix}
\stopformula

不妨验证一下，如果 $n = k + 1$，结果便为 $\frac{M - 1}{N - 1}$，理应如此。

但是，此时我们对 (\in[3-56]) 的兴趣不在于数值，而在于认识这一结果的逻辑。我们先把它特化为最简单但并非不重要的情况。假设我们从一个含有 $N = 4$ 个球且其中有 $M = 2$ 个是白球的瓮中取 $n = 3$ 次球，然后问，在第二次和第三次抽取时，至少有一次取到红球，这一信息会如何影响第一次取到红球的概率。在 $N = 4$、$M = 2$、$n = 3$、$k = 1$ 时，(\in[3-56]) 可以给出这个概率：

\placeformula[3-57]
\startformula
P(R_1|R_2 + R_3, B) = \frac{6 - 2}{12 - 2} = \frac{2}{5} = \left(\frac{1}{2}\right)\frac{1 - \displaystyle\frac{1}{3}}{1 - \displaystyle\frac{1}{6}}
\stopformula

最后的形式与 (\in[3-51]) 相应。将这个概率与之前算出的概率

\placeformula[3-58]
\startformula
P(R_1|B) = \frac{1}{2}\quad\quad P(R_1|R_2B) = P(R_2|R_1B) = \frac{1}{3}
\stopformula

令人吃惊的是

\placeformula[3-59]
\startformula
P(R_1|R_{\text{later}}B) > P(R_1|R_2B)
\stopformula

很多人会认为这个不等式的方向弄反了；亦即，知道了红球在后续抽取中至少被抽取一次，这一信息对于第一次抽取到红球的机会的消减程度要比信息 $R_2$ 更大才对。但是，在这种情况下，由于数字较小，我们可以直接检验 (\in[3-51]) 的计算。使用扩展的加法规则 (\in[2-82]) 对 $P(R_{\text{later}}|B)$ 进行展开，可得：

\placeformula[3-60]
\startformula
\displaystyle
\startmathalignment
\NC P(R_{\text{later}}|B) \NC = P(R_2|B) + P(R_3|B) - P(R_2R_3|B)\NR
\NC \NC = \frac{1}{2} + \frac{1}{2} - \frac{1}{2}\times\frac{1}{3} = \frac{5}{6}\NR
\stopmathalignment
\stopformula

我们也可以将 $R_{\text{later}}$ 分解为互相独立的命题，然后计算

\placeformula[3-61]
\startformula
\displaystyle
\startmathalignment
\NC P(R_{\text{later}}|B) \NC = P(R_2W_3|B) + P(W_2R_3|B) - P(R_2R_3|B)\NR
\NC \NC = \frac{1}{2}\times\frac{2}{3} + \frac{1}{2}\times\frac{2}{3} + \frac{1}{2}\times\frac{1}{3} = \frac{5}{6}\NR
\stopmathalignment
\stopformula

(\in[3-57]) 中的已经被采用不同的方式计算了三次，结果都是相同的。如果这三个结果有所不同，我们就会发现我们的规则——我们试图通过第 2 章中的 Cox 函数方程去证明的那种——存在不一致性。对于\quotation{一致性}的实际意义而言，这是一个很好的例子，它表明了，倘若我们的规则不具备这个性质，那么我们就会陷入困境。

同样地，可以通过单独计算来检验 (\in[3-51]) 中的分子：

\placeformula[3-62]
\startformula
\displaystyle
\startmathalignment
\NC P(R_{\text{later}}|R_1B) \NC = P(R_2|R_1B) + P(R_3|R_1B) - P(R_2R_3|R_1B)\NR
\NC \NC = \frac{1}{3} + \frac{1}{3} - \frac{1}{3}\times 0 = \frac{2}{3}
\stopmathalignment
\stopformula

于是，(\in[3-57]) 这个结果就可以确定了。因此，除了接受 (\in[3-59]) 这个不等式，并且尝试在直觉上理解它之外，我们别无选择。不妨像下面这样推理。信息 $R_2$ 使得对于首次抽取有效的红球数量减一，它也使得对于首次有效的瓮内球的总数减一，即 $P(R_1|R_2B) = \frac{M - 1}{N - 1} = \frac{1}{3}$。信息 $R_{\text{later}}$ 将对于首次抽取有效的\quotation{红球的有效个数}减了不止一，但是它使得对于首次抽取有效的瓮内球数减去了二（因为它让机器人确信后面还有两次抽取，从而瓮内有两个球被取出）。因此，我们试着将 (\in[3-57]) 解释为

\placeformula[3-63]
\startformula
P(R_1|R_{\text{later}}B) = \frac{(M)_{\text{eff}}}{N - 2}
\stopformula

尽管我们还不能充分肯定这是什么意思。给定 $R_{\text{later}}$，可以确定至少有一个红球被从瓮中移除，并且根据乘法规则，两个红球被移除的概率为

\placeformula[3-64]
\startformula
\startmathalignment
\NC P(R_2R_3|R_{\text{later}}B) \NC = \frac{R_2R_3R_{\text{later}}|B}{P(R_{\text{later}})} = \frac{P(R_2R_3|B)}{P(R_{\text{later}}|B)}\NR
\NC \NC = \frac{\displaystyle\frac{1}{2}\times\frac{1}{3}}{\displaystyle\frac{5}{6}} = \frac{1}{5}\NR
\stopmathalignment
\stopformula

由于 $R_2R_3$ 蕴含 $R_{\text{later}}$；亦即，存在布尔代数关系 $(R_2R_3R_{\text{later}} = R_2R_3)$。在直觉上，给定 $R_{\text{later}}$，则两个红球被移除的概率为 $\frac{1}{5}$，因此被移除的红球的有效数字为 $1 + \frac{1}{5} = \frac{6}{5}$。剩下的那个红球，它的\quotation{有效}数字为 $\frac{4}{5}$。实际上，(\in[3-63]) 变为

\placeformula[3-65]
\startformula
P(R_1|R_{\text{later}}B) = \frac{\displaystyle\frac{4}{5}}{2} = \frac{2}{5}
\stopformula

它更为符合我们的意图 (\in[3-57])，只是不够直观。

\section{期望}

另一种对待这一结果的方式对于我们的直觉更有冲击力，而且也很大程度上能够将当前的问题一般化。我们很难想象读者会不熟悉期望这个概念，但是在这本书里它是第一次出现，因此我们要花点时间给它一个定义。如果一个变化的量 $X$，在 $n$ 个相互独立且详尽的情况下，它可取的值为 $(x_1,\cdots,x_n)$，并且机器人为这些情况赋予了概率 $(p_1,p_2,\cdots,p_n)$，则

\placeformula[3-66]
\startformula
\langle X\rangle = E(X) = \sum_{i = 1}^np_ix_i
\stopformula

被称为 $X$ 的{\bf 期望}（在旧有文献里，也称为数学期望或期望值）。这是 $X$ 可能取值的加权平均，权重系数即 $X$ 取这些值的概率。统计学家和数学家通常使用 $E(X)$ 这种记法；但是物理学家已经将 $E$ 用于表示能量和电场了，因此他们使用 $\langle X\rangle$ 这种记法。我们是两种记法都用；它们的含义相同，只是有时一种会比另一种更易读。

如同大多数标准的术语有其悠久的历史那样，\quotation{期望}，在我们看来，这个术语有些怪异；它并非是任何人\quotation{期盼}着去发现的数值。实际上，它通常被理解为一个不可能的值。我们之所以继续用这个术语，是因为数百年前就已经用这个术语了。

给定 $R_{\text{later}}$，第一次取球的时候瓮内红球个数的期望是多大？存在与 $R_{\text{later}}$ 吻合的三个相互独立的可能性：

\placeformula[3-67]
\startformula
R_2W_3\quad W_2R_3\quad R_2R_3
\stopformula

它们分别与 $M = (1, 1, 0)$ 相对应，并且它们概率就像 (\in[3-64]) 和 (\in[3-65]) 那样：

\placeformula[3-68]
\startformula
P(R_2W_3|R_{\text{later}}B) = \frac{R_2W_3|B}{P(R_{\text{later}}|B)} = \frac{\displaystyle\frac{1}{2}\times\frac{2}{3}}{\displaystyle\frac{5}{6}} = \frac{2}{5}
\stopformula

\placeformula[3-69]
\startformula
P(W_2R_3|R_{\text{later}}B) = \frac{2}{5}
\stopformula

\placeformula[3-70]
\startformula
P(R_2R_3|R_{\text{later}}B) = \frac{1}{5}
\stopformula

因此

\placeformula[3-71]
\startformula
\langle M\rangle = 1\times\frac{2}{5} + 1\times\frac{2}{5} + 0\times\frac{1}{5} = \frac{4}{5}
\stopformula

这样我们在直觉上所谓的 $M$ 的\quotation{有效}值，实际上就是 $M$ 的期望。

现在，我们能够以中肯的方式给出 (\in[3-63]) 的阐释：若红球的比例 $F = \frac{M}{N}$ 已知，那么应用 Bernoulli 瓮的规则，可得 $P(R_1|B) = F$。若 $F$ 未知，则红球的概率就是 $F$ 的期望：

\placeformula[3-72]
\startformula
P(R_1|B) = \langle F\rangle \equiv E(F)
\stopformula

如果 $M$ 和 $N$ 皆为未知，这个期望就是 $M$ 和 $N$ 的联合概率分布。

概率在数值上等于分数的期望，这一点将会被证明是一个广义规则，它在成千上万的更为复杂的情形下依然成立，从而为现实预测提供了最为有用也最为普遍的规则之一。更为一般的结果 (\in[3-56]) 也能按照 (\in[3-72]) 的方式算出，我们将这个作为练习留给读者去证明。

\section{其他形式和扩展}

超几何分布 (\in[3-22]) 有多种形式，它的九个阶乘可以像下面这样组织成二项式系数：

\placeformula[3-73]
\startformula
h(r|N,M,n) = \frac{\startpmatrix n\NR r\NR\stoppmatrix\startpmatrix N - n\NR M - r\NR\stoppmatrix}{\startpmatrix N\NR M\NR\stoppmatrix}
\stopformula

但是对于 $M$ 和 $n$ 的交换，其对称性仍不够明显；要看清这一点，我们必须将 (\in[3-22]) 或 (\in[3-73]) 完全展开。

为了方便记忆，我们也可以基于一种更为对称的形式重写 (\in[3-22])：从含有 $R$ 个红球和 $W$ 个白球的瓮中，在 $n = r + w$ 次的抽取中，恰好取到 $r$ 个红球和 $w$ 个白球的概率。这个概率为

\placeformula[3-74]
\startformula
h(r|N,M,n) = \frac{\startpmatrix R\NR r\NR\stoppmatrix\startpmatrix W\NR w\NR\stoppmatrix}{\startpmatrix R + W\NR r + w\NR\stoppmatrix}
\stopformula

这种形式的通用性更好。假设，瓮内的球不止两种颜色，而是 $k$ 种，颜色 1 为 $N_1$，颜色 2 为 $N_2$，$\cdots$，颜色 $k$ 为 $N_k$。在 $n = \sum r_i$ 次抽取中，取到 $r_1$ 个颜色 1 的球、$r_2$ 个颜色 2 的球、$\cdots$，$r_k$ 个颜色 $k$ 的球的概率就是广义超几何分布：

\placeformula[3-75]
\startformula
h(r_1\cdots r_k|N_1\cdots N_k) = \frac{\startpmatrix N_1\NR r_1\NR\stoppmatrix\cdots\startpmatrix N_k\NR r_k\NR\stoppmatrix}{\startpmatrix \sum N_i\NR\sum r_i\NR\stoppmatrix}
\stopformula

对此，读者可自行验证。

\section{作为数学工具的概率}

根据结果 (\in[3-75])，可以获得一些由二项式系数构成的等式。例如，我们可以决定不区分颜色 1 和 2；亦即，球的颜色有两种皆称为\quotation{$a$}。然后，根据 (\in[3-75])，在一方面可得

\placeformula[3-76]
\startformula
h(r_a,r_3,\cdots,r_k|N_a,N_3,\cdots,N_k) = \frac{\startpmatrix N_a\NR r_a\NR\stoppmatrix\startpmatrix N_3\NR r_3\NR\stoppmatrix\cdots\startpmatrix N_k\NR r_k\NR\stoppmatrix}{\startpmatrix \sum N_i\NR \sum r_i\NR\stoppmatrix}
\stopformula

其中，

\placeformula[3-77]
\startformula
N_a = N_1 + N_2\quad\quad r_a = r_1 + r_2
\stopformula

但是对于满足 (\in[3-77]) 的 $r_1$ 和 $r_2$ 的任何值皆可构成 $r_a$，因此在另一方面可得

\placeformula[3-78]
\startformula
h(r_a,r_3,\cdots,r_k|N_a,N_3,\cdots,N_k) = \sum_{r_1 = 0}^{r_a}h(r_1,r_a - r_1, r_3,\cdots,r_k|N_1,\cdots,N_k)
\stopformula

然后比较 (\in[3-76]) 和 (\in[3-78])，可得等式

\placeformula[3-79]
\startformula
\startpmatrix N_a\NR r_a\NR\stoppmatrix = \sum_{r_1 = 0}^{r_a}\startpmatrix N_1\NR r_a\NR\stoppmatrix\startpmatrix N_2\NR r_a - r_1\NR\stoppmatrix
\stopformula

沿用这种方式，我们可以导出规模更大且更复杂的二项式系数构成的等式。例如

\placeformula[3-80]
\startformula
\startpmatrix N_1 + N_2 + N_3\NR r_a\NR\stoppmatrix = \sum_{r_1 = 0}^{r_a}\sum_{r_2 = 0}^{r_1}\startpmatrix N_1\NR r_1\NR\stoppmatrix\startpmatrix N_2\NR r_2\NR\stoppmatrix\startpmatrix N_3\NR r_a - r_1 - r_2\NR\stoppmatrix
\stopformula

在许多情况下，概率化的推理是推导纯数学结果的利器；Feller (1950, 第 2、3 章) 给出了许多这样的例子。本书后面的章节也有一些。

\section{二项分布}

尽管数学上相当复杂，但是超几何分布却来源于一个概念上清晰且简单的问题；仅存在有限种可能性，并且以上所有结果也与所述的问题相符。作为对数学上更简单但概念上更难的一个问题的介绍，我们要考查超几何分布的极限形式。

超几何分布之所以复杂，是因为它考虑的是瓮内之物的变化；知悉任何一次抽取的结果，都会改变任何其他次取到红球的概率。但是，如果瓮内的球数 $N$ 远大于抽取次数（$N \gg n$），那么这个概率的改变就会非常小，并且在极限 $N\rightarrow\infty$ 情况下，我们应当有一个更简单的结果独立于此类依赖。为了验证这一点，我们将超几何分布 (\in[3-22]) 写为

\placeformula[3-81]
\startformula
h(r|N,M,n) = \frac{\left[\displaystyle\frac{1}{N^r}\startpmatrix M\NR r\NR\stoppmatrix\right]\left[\displaystyle\frac{1}{N^{n - r}}\startpmatrix N - M \NR n - r\NR\stoppmatrix\right]}{\left[\displaystyle\frac{1}{N^n}\startpmatrix N\NR n\NR\stoppmatrix\right]}
\stopformula

第一个因子是

\placeformula[3-82]
\startformula
\frac{1}{N^r}\startpmatrix M\NR r\NR\stoppmatrix = \frac{1}{r!}\frac{M}{N}\left(\frac{M}{N} - \frac{1}{N}\right)\left(\frac{M}{N} - \frac{2}{N}\right)\cdots\left(\frac{M}{N} - \frac{r - 1}{N}\right)
\stopformula

在极限 $N\rightarrow\infty$、$M\rightarrow\infty$ 以及 $\frac{M}{N}\rightarrow f$ 的情况下，有

\placeformula[3-83]
\startformula
\frac{1}{N^r}\startpmatrix M\NR r\NR\stoppmatrix\rightarrow\frac{f^r}{r!}
\stopformula

同样地

\placeformula[3-84]
\startformula
\frac{1}{N^{n - r}}\startpmatrix N - M\NR n - r\NR\stoppmatrix\rightarrow\frac{(1 - f)^{n - r}}{(n - r)!}
\stopformula

\placeformula[3-85]
\startformula
\frac{1}{N^n}\startpmatrix N\NR n - r\NR\stoppmatrix\rightarrow\frac{1}{n!}
\stopformula

原则上，我们应当取 (\in[3-81]) 的积的极限，而不是极限的积。但是在 (\in[3-81]) 里所定义的各个因子，它们的极限是相互独立的，因此所得结果与极限的积相同。如此，超几何分布便转化为

\placeformula[3-86]
\startformula
h(r|N, M, n)\rightarrow b(r|n, f) \equiv\startpmatrix n\NR r\NR\stoppmatrix f^r(1 - f)^{n - r}
\stopformula

将这一分布称为{\bf 二项}分布。之所以如此命名，是因为那个生成函数 (\in[3-24]) 现在可约化为

\placeformula[3-87]
\startformula
G(t) \equiv \sum_{r = 0}^n b(r|n, f)t^r = (1 - f + ft)^n
\stopformula

这是牛顿二项式定理的一个特例。

\strut 图 \in[fig-3-1] 比较了三种超几何分布与二项分布。这三种超几何分布，相关参数为 $N = 15, 30, 100$，$\frac{M}{N} = 0.4$ 以及 $n = 10$。二项分布的相关参数为 $n = 10$ 和 $f = 0.4$。它们的波峰皆位于 $r = 4$ 时出现，并且所有的分布都有相同的一阶矩 $\langle r\rangle = E(r) = 4$，但二项分布更宽。

\placefigure[here][fig-3-1]{\switchtobodyfont[9pt]{$N = 15, 30, 100, \infty$} 时的超几何分布}{\externalfigure[figures/3-1.pdf][width=.8\textwidth]}

\indentation $N = 15$ 对应的超几何分布在 $r = 0$ 与 $r > 6$ 时为 $0$，这是因为从只包含 6 个红球和 9 个白球的瓮中取 10 个球，取到的红球数量不可能取到比 1 个更少或者比 6 个更多。当 $N > 100$ 时，超几何分布便于二项分布相当吻合，对于大部分目的而言，这两个用哪一个都可以。第 7 章会给出二项分布的分析学性质。在第 9 章中，我们发现二项分布不过是有限采样空间中的单纯的组合推理罢了，这种推理与显著性测试有关，见方程 (\in[9-46])。

我们可以对广义超几何分布 (\in[3-75]) 执行相似的求极限过程。若 $N_i\rightarrow\infty$，证明以分数

\placeformula[3-88]
\startformula
f_i \equiv \frac{N_i}{\sum N_j}
\stopformula

趋向于常量的极限的情况下，(\in[3-75]) 会转变为{\bf 多项分布}

\placeformula[3-89]
\startformula
m(r_1\cdots r_k|f_1\cdots f_k) = \frac{r!}{r_1!\cdots r_k!}f_1^{r_1}\cdots f_k^{r_k}
\stopformula

其中 $r \equiv \sum r_i$。如同 (\in[3-87])，可以定义 $(k - 1)$ 个变量的生成函数，由该函数可以证明 (\in[3-89]) 是被正确地归一化了的，并且可推导出许多其他有用的结果。

\startExercise
假设瓮内含有 $N = \sum N_i$ 个球，$N_1$ 个颜色 $1$ 的，$N_2$ 个颜色 $2$ 的，$\cdots$，$N_k$ 个颜色 $k$ 的。从瓮中无放回地取出 $m$ 个球；每种颜色的球至少取一个的概率是多大？假设 $k = 5$，所有的 $N_i = 10$，要得到一个颜色完全的集合？
\stopExercise

\startExercise
假设上一个习题中的 $k$ 一开始为未知，但是已知瓮内刚好有 50 个球。从瓮中取出 20 个球，它们有三种不同的颜色；现在，该如何获知 $k$ 值？根据演绎推理，我们可以确定 $3 \le k \le 33$，但是能否可以更窄一些的限定 $k_1\le k \le k_2$ 以更好地逼向 $k$？

\noindent {\bf 提示}：这个问题超出了本章的采样理论，因为像大多数真实的科学问题一样，答案在一定程度上依赖于我们的常识判断；无论如何，我们的概率论规则擅长处理此事，并且具有合理常识的人不可能会得出不同的结论。
\stopExercise

\startExercise
有 $M$ 个瓮，编号从 1 到 $M$。有 $M$ 个球，编号也从 1 到 $M$。将这些球掷入瓮中。如果球的编号与瓮的编号相同，就有一次匹配。证明至少有一次匹配的概率为

\placeformula[3-90]
\startformula
h = \sum_{k = 1}^M \frac{(-1)^{k + 1}}{k!}
\stopformula

随着 $M \rightarrow M$，上式会收敛为 $1 - \frac{1}{e} = 0.632$。这个结果让许多人吃惊，因为无论 $M$ 有多大，对于不匹配，总是保持着可观的概率。
\stopExercise

\startExercise
将 $N$ 个球掷入 $M$ 个瓮内；显然有 $M^N$ 种方式。如果机器人公平地考虑这些方式，那么每个瓮内至少获得一个球的概率是多大？
\stopExercise

\defineenumeration
  [remark]
  [text=,headstyle=bold, way=bychapter, alternative=serried, width=fit,
  headcommand={习题 \headnumber[chapter].\headnumber[remark]}]

\remark \input knuth

\chapter{aaa}

\remark