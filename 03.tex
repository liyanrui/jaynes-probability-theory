\chapter[quantitative-rules]{初等采样理论}

至此，我们所拥有并且有效的数学基础由基本的乘法和加法规则构成：

\placeformula[3-1]
\startformula
P(AB|C) = P(A|BC)P(B|C) = P(B|AC)P(A|C)
\stopformula

\placeformula[3-2]
\startformula
P(A|B) + P(\itbar{A}|B) = 1
\stopformula

从它们可以导出扩展的加法规则：

\placeformula[3-3]
\startformula
P(A + B|C) = P(A|C) + P(B|C) - P(AB|C)
\stopformula

使用一致性的祈求 (\Roman{3}c)，亦即无差异性原理：如果一组假设 $(H_1,H_2,\cdots,H_N)$ 在背景信息 $B$ 上相互独立且详尽，并且 $B$ 不会偏好这些假设中的任何一个，那么

\placeformula[3-4]
\startformula
P(H_i|B) = \frac{1}{N}\quad\quad 1\le i \le N
\stopformula

从 (\in[3-3]) 和 (\in[3-4]) 又可以推导出 Bernoulli 瓮规则：如果 $B$ 指定了 $A$ 在 $M$ 个假设所构成的子集上为真，而在剩下的 $(N - M)$ 假设所构成的子集为假，那么

\placeformula[3-5]
\startformula
P(A|B) = \frac{M}{N}
\stopformula

意识到概率论在内容上有多少能够仅仅从此式推导出来，这一点非常重要。

实际上，当前所教授的概率论的全部内容，加上许多经常重要结果——它们经常被视为超出了概率论范围，可以从上述基础推导出来。接下来的几章内容会给出一些细节，而后在第 11 章，继续发展我们的机器人的大脑，让它能够充分理解高级应用所需要的另外一些原理。

在本章中，我们的概率论，它的第一个应用与我们之后所期望达到的严肃科学推断相比，相当简单和幼稚。无论如何，我们从细节上来考虑这些，原因并不仅仅是面向教学。对于这些最为简单的应用，对其逻辑的误解，是几十年来阻碍科学推断发展进程——也包含科学本身——的主要原因之一。因而我们强烈建议读者，即使你早已熟悉初等采样理论，在处理更为复杂的问题之前，也应当认真消化本章内容。

\section{无放回采样}

为了让 Bernoulli 瓮更为明确，我们定义了以下命题：

\definedescription[mydefinition][location=left, before=, after=, headstyle=\tf, width=broad, distance=0.25em]
\def\myproposition#1{\hbox to 1.5em{#1}$\equiv$}
\mydefinition{\myproposition{$B$}} 有个瓮，里面有 $N$ 个球。它们的编号是 $(1,2,\cdots,N)$，其中有 $M$ 个球是红色的，其余的是白色的，$0\le M \le N$。除此之外，这些球的各方面都相同。闭着眼从瓮中取一个球，观察并记录它的颜色，再把它放回去。重复这一过程 $n$ 次，$0\le n \le N$。\par
\mydefinition{\myproposition{$R_i$}} 第 $i$ 次取的球为红色。\par
\mydefinition{\myproposition{$W_i$}} 第 $i$ 次取的球为白色。\par

根据 $B$，能够取的球仅有红色和白色，有

\placeformula[3-6]
\startformula
P(R_i|B) + P(W_i|B) = 1\quad\quad 1\le i \le N
\stopformula

它等同于说，在知识 $B$ 所创立的\quotation{逻辑环境}里，命题 $R_i$ 与 $W_i$ 的关系是互反的，即

\placeformula[3-7]
\startformula
\itbar{R_i} = W_i\quad\quad \itbar{W_i} = R_i
\stopformula

并且，对于第一次取球，(\in[3-5]) 变为

\placeformula[3-8]
\startformula
P(R_i|B) = \frac{M}{N}
\stopformula

\placeformula[3-9]
\startformula
P(W_i|B) = 1 - \frac{M}{N}
\stopformula

现在来看这意味着什么。概率赋值 (\in[3-8]) 和 (\in[3-9]) 所断言的并非瓮的物理属性或其内容；它们描述的是机器人在取球之前所具备的{\bf 知识状态}。实际上，倘若机器人的知识状态与上面定义的 $B$ 有所不同（例如，它知道红球和白球在瓮内的位置，或者它不知道 $N$ 和 $M$ 多大），那么它为 $R_i$ 和 $W_i$ 所赋的概率必然与 (\in[3-8]) 和 (\in[3-9]) 不同，而瓮的现实属性却不会变。

因此，称用瓮来做实验以\quotation{验证}(\in[3-8])，这种说法，如同在狗身上做实验来验证一个孩子对他的狗的喜爱，毫无逻辑可言。眼下，我们关心的是来自不完备信息的一致性推理所涉及的逻辑，而不是瓮中取出什么东西这一物理现象的断言（任何情况下，着都是不可能做到的，因为信息 $B$ 不完备）。

最终，我们的机器人将会能够作出一些非常肯定的现实预测。这些预测能够接近，但是（除非在一些退化的情况下）并不能真正达到逻辑演绎的确定性；但是，在我们能够说出什么量能够被准确预测并且为此需要何种信息之前，理论还需要进一步发展。换言之，由机器人在不同知识状态下所赋的概率与实验中可观察到的事实，这二者之间的关系可能并非随意建立的；我们有充分的理由仅仅使用可由概率论规则推导出来的那些关系，这是是我们现在正要去做的的事。

当我们向机器人索取与第二次抽取相关的概率时，机器人的知识状态便会出现变化。例如，开始的两次取球，取到的皆为红球的概率是多大？根据乘法规则，这个概率为

\placeformula[3-10]
\startformula
P(R_1R_2|B) = P(R_1|B)P(R_2|R_1B)
\stopformula

在最后一个因子中，机器人必须考虑第一所取的红球已经从瓮中移除，因此瓮中所剩下的 $(N - 1)$ 个球中有 $(M - 1)$ 个是红球。因而

\placeformula[3-11]
\startformula
P(R_1R_2|B) = \frac{M}{N}\frac{M - 1}{N - 1}
\stopformula

沿着这一思路，前 $r$ 次连续取球，所取之球皆为红球的概率为

\placeformula[3-12]
\startformula
\startmathalignment[style=\displaystyle]
\NC P(R_1R_2\cdots R_r|B) \NC = \frac{M(M - 1)\cdots (M - r + 1)}{N(N - 1)\cdots (N - r + 1)}\NR
\NC \NC = \frac{M!(N - r)!}{(M - r)!N!}\quad\quad r\le M\NR
\stopmathalignment
\stopformula

倘若我们用伽马函数关系 $n! = \Gamma(n + 1)$ 来定义阶乘，则限制条件 $r\le M$ 并非必须，因为当 $r > M$ 时，负整数的阶乘结果为无穷大，于是 (\in[3-12]) 的结果自动为 0。

前 $w$ 次取球，取到的球皆为白球的概率与 (\in[3-12]) 相似，只是要将 $M$ 换成 $(N - M)$：

\placeformula[3-13]
\startformula
P(W_1W_2\cdots W_w|B) = \frac{(N - M)!(N - w)!}{(N - M - w)!N!}
\stopformula

假设前 $r$ 次取球，取到的皆为红球，那么在 $(r + 1, r + 2, \cdots,r + w)$ 次取球，取到的皆为白球的概率，需要在 (\in[3-13]) 的基础上考虑 $N$ 和 $M$ 已分别缩减为 $(N - r)$ 和 $(M - r)$：

\placeformula[3-14]
\startformula
P(W_1W_2\cdots W_w|R_1\cdots R_rB) = \frac{(N - M)!(N - r - w)!}{(N - M - w)!(N - r)!}
\stopformula

因此，通过乘法规则，在 $n$ 次取球的过程中，取出 $r$ 个红球之后，又取出 $w = n - r$ 个白球的概率，由 (\in[3-12]) 和 (\in[3-14]) 可得

\placeformula[3-15]
\startformula
P(R_1\cdots R_rW_{r + 1}\cdots W_n|R_1\cdots R_rB) = \frac{M!(N - M)!(N - n)!}{(M - r)!(N - M - w)!N!}
\stopformula

$(N - r)!$ 项被约掉了。

尽管这个结果基于红球和白球特定的取出顺序推导出来，但是以任何特定顺序在 $n$ 次取球中取出 $r$ 个红球，概率都是相同的。为了看清这一点，需要采用下面的方式

\placeformula[3-16]
\startformula
\frac{M!}{(M - r)!} = M(M - 1)\cdots (M - r + 1)
\stopformula

对 (\in[3-15]) 的其他比率作更完全地展开。于是 (\in[3-15]) 的右部变为

\placeformula[3-17]
\startformula
\frac{M(M - 1)\cdots (M - r + 1)(N - M)(N - M - 1)\cdots (N - M - w + 1)}{N(N - 1)\cdots (N - n + 1)}
\stopformula

现在假设 $r$ 个红球和 $(n - r) = w$ 个白球以任意次序被取出。这种情况的概率是 $n$ 个因子的积；每次取到红球，就会存在一个因子 $\frac{(\text{瓮内红球的数量})}{(\text{瓮内球的总数})}$，并且，同样可以为取到白球写出相似的因子。每次取球，瓮内的球的数量便会少一个；因而第 $k$ 次取球，$(N - k + 1)$ 就会在分母中出现，无论在取这个球之前所取的那些球是什么颜色。

就在第 $k$ 个红球被取出之前，无论这个球是在第 $k$ 次还是第 $k$ 次之后的任意一次取球时被取出，瓮内剩下 $(M - k + 1)$ 个红球；因而，取第 $k$ 个球，就相当于在分子上放了一个因子 $(M - k + 1)$。就在第 $k$ 个白球被取出之前，瓮内剩下 $(N - M - k + 1)$ 个红球；因而，取第 $k$ 个球，无论这个球是在第 $k$ 次还是第 $k$ 次之后的任意一次取球时被取出，就相当于在分子上放了一个因子 $(N - M - k + 1)$。因而，此时 $n$ 个球皆已取出，其中 $r$ 个是红球，我们在分子和分母上累积的因子与 (\in[3-17]) 相同；不同的取球次序仅仅是排列分子中因子的次序罢了。因此，在 $n$ 次取球的过程中，以任意次序刚好取出 $r$ 个红球，这种情况的概率皆由 (\in[3-15]) 而定。

务必注意，在这个结果中，乘法规则是以一种特殊的方式展开的，这种方式向我们揭示了如何将计算组织成因子的乘积，{\bf 给定之前的所有取球的结果}，每一个因子都是一次特定的取球结果的概率。但是还有许多其他方式可以将这个乘法规则展开，每一种展开结果都能够给出以不止所有之前的取球结果为条件的因子；所有的这些计算一定会得到相同的结果，这是一个事实，一个非常重要的一致性性质，是第 \in[quantitative-rules] 章的推导所要致力确定的。

接下来，我们问：在 $n$ 次取球的过程中，无关乎次序，恰好取到 $r$ 个红球的概率是多大？红球与白球出现不同的次序的可能性相互独立，因此我们要把这些可能性叠加起来；由于每一项都等于 (\in[3-15])，所以只需要用二项式系数

\placeformula[3-18]
\startformula
\startpmatrix n\NR r\NR\stoppmatrix = \frac{n!}{r!(n - r)!}
\stopformula

乘以 (\in[3-15])。二项式系数表示 $n$ 次取球恰好取到 $r$ 个红球的可能取法的数量，我们将其称为事件 $r$ 的{\bf 多样性}。例如，在三次取球过程中取到三个红球，只有一种取法，因为

\placeformula[3-19]
\startformula
\startpmatrix 3\NR 3\NR\stoppmatrix = 1
\stopformula

这种取法是 $R_1R_2R_3$，亦即事件 $r = 3$ 的多样性为 1。但是，要从三次取球过程中获得两个红球，取法有三种，因为

\placeformula[3-20]
\startformula
\startpmatrix 3\NR 2\NR\stoppmatrix = 3
\stopformula

这三种取法分别是 $R_1R_2W_3$、$R_1W_2R_3$ 以及 $W_1R_2R_3$，因此事件 $r = 2$ 的多样性为 $3$。

\startExercise
为什么 (\in[3-18]) 的多样性因子不是 $n!$？毕竟，我们是从规定小球不仅有颜色而且也有标签 $(1,2,\cdots,N)$，因此红球本身的不同排列——为 (\in[3-18]) 的分母贡献了 $r!$，造成有区别的 $n$ 次取球的结果。

提示：在 (\in[3-15]) 中，我们没有指定哪些红球和哪些白球要被抽取。
\stopExercise

计算 (\in[3-15]) 和 (\in[3-18]) 的积，许多阶乘可以重新组织成三个二项式系数。定义 $A\equiv$\quotation{以任意次序取 $n$ 个球，其中有 $r$ 个红球}，还有函数

\placeformula[3-21]
\startformula
h(r|N, M, n) \equiv P(A|B)
\stopformula

这样就有

\placeformula[3-22]
\startformula
h(r|N, M, n) = \frac{\startpmatrix M\NR r\NR\stoppmatrix\startpmatrix N - M\NR n - r\NR\stoppmatrix}{\startpmatrix N\NR n\NR\stoppmatrix}
\stopformula

我们应当将其缩写为 $h(r)$。通过约定 $x! = \Tau(x + 1)$，当 $r > M$ 或 $r > n$ 或者 $(n - r) > (N - M)$ 时，$h(r)$ 就会自动变为 $0$，理应如此。

在此，我们动用了记法上的小伎俩，原因在附录 B 部分给出了解释。要点是，正式的概率符号 $P(A|B)$，用的是大写字母 $P$，参数 $A$ 和 $B$ 总是表示命题，它们可以是相当复杂的语言陈述。若想用普通数字作为参数，那么为了一致性，我们应当定义新的函数符号，诸如 $h(r|N, M, n)$。坚持使用 $P(r|NMn)$，忽视了 $A$ 和 $B$ 定性方面的约定，会因为曲解了这些方程的意思而导致严重的错误（例如以后要讨论的一些生僻的悖论）。不过，如第 \in[quantitative-rules] 章所述，我们通过使用 $p(A|B)$ 或 $p(r|n)$ 这样的使用小写字母 $p$ 的概率符号来迎合现代的工作，在这些符号中，我们允许使用命题或代数变量作为参数；在这种情况下，它们的含义由语境而定。

基本结果 (\in[3-22]) 被称为\quotation{超几何分布}，因为它与高斯超几何函数的幂级数形式

\placeformula[3-23]
\startformula
F(a,b,c;t) = \sum_{r = 0}^{\infty}\frac{\Tau(a + r)\Tau(b + r)\Tau(c)}{\Tau(a)\Tau(b)\Tau(c + r)}\frac{t^r}{r!}
\stopformula

的系数有关。如果 $a$ 和 $b$ 中有一个为负整数，这个级数会收敛，因此它是个多项式。很容易验证，{\bf 生成函数}

\placeformula[3-24]
\startformula
G(t) \equiv \sum_{r = 0}^nh(r|N, M, n)t^r
\stopformula

等于

\placeformula[3-25]
\startformula
G(t) = \frac{F(-M,-n,c;t)}{F(-M,-n,c;1)}
\stopformula

其中 $c = N - M - n + 1$。显然 $G(1) = 1$，根据 (\in[3-24])，这个等式描述的是超几何分布被正确地归一化了。在 (\in[3-25]) 的结果中，$G(t)$ 满足二阶超几何微分方程，并且有一些与计算有用的其他性质。

尽管超几何分布 $h(r)$ 看上去很复杂，但是它有着一些令人惊讶的简单性质。通过设定 $h(r') = h(r' - 1)$，$r$ 最有可能的值被发现位于一个单位之内。我们发现的是

\placeformula[3-26]
\startformula
r' = \frac{(n + 1)(M + 1)}{N + 2}
\stopformula

如果 $r'$ 是整数，那么 $r'$ 和 $r' - 1$ 都是最有可能的值。如果 $r'$ 并非整数，那么就会有唯一的最可能的值是

\placeformula[3-27]
\startformula
\hat{r}=\text{INT}(r')
\stopformula

即比 $r'$ 小的整型值。因而采样取球中红球的最有可能的分数 $f = \frac{r}{m}$，如同我们的直觉所期盼的那样，它近似等于分数 $F = \frac{M}{N}$，后者是瓮内原有的红球的个数与球的总数的比值。在现实预测方面，这是我们的第一个粗略的示例：我们的信息所指定的量 $F$ 与现实实验所测定的量 $f$ 之间的关系，而这个关系是根据概率论推导出来的。

超几何分布 $h(r)$ 的宽度给出了精确性指标，利用这个指标，机器人可以判定 $r$。许多这样的问题，它们的答案可通过计算{\bf 累加概率}而得到，即寻找 $R$ 个或更少的红球的概率。如果 $R$ 是整数，累加概率就是

\placeformula[3-28]
\startformula
H(R) \equiv\sum_{r = 0}^Rh(r)
\stopformula

不过，为了后文中给出正式的原因，我们将 $H(x)$ 定义成一种阶梯函数，$x$ 值为所有非负实数；因而 $H(x)=H(R)$，其中 $R = \text{INT}(x)$ 是不大于 $x$ 的最大整数。

对于诸如 $h(r)$ 这样的概率分布，将其{\bf 中值}定义为一个数 $m$，命题 $r < m$ 和 $r > m$ 被赋予相等的概率。严格而言，根据这个定义，一个离散的分布通常没有中值。倘若存在一个整数 $R$，满足 $H(R - 1) = 1 - H(R)$，并且 $H(R) > H(R - 1)$，则 $R$ 是唯一的中值。倘若有一个整数 $R$，满足 $H(R) = \frac{1}{2}$，则 $R\le r < R'$ 之内的任意 $r$ 都是中值，其中 $R'$ 是 $H(x)$ 的下一个比较高的跃点；否则不存在中值。

但是对于大多数目的而言，我们可能要宽容一些，以接近定义为准。如果 $n$ 是足够大，那么便有理由认为能够让 $H(R)$ 非常接近 $\frac{1}{2}$ 的 $R$ 为\quotation{中值}。以相同的宽容精神，使得 $H(R)$ 非常接近 $\frac{1}{4}$、$\frac{3}{4}$ 的 $R$ 值，可将其称为\quotation{低四等分}和\quotation{高四等分}。如果 $n \gg 10$，使得 $H(R)$ 非常接近 $\frac{k}{10}$ 的 $R$ 值，我们称为\quotation{第 $k$ 个十等分}，以此类推。当 $n \rightarrow \infty$，这些宽松的定义就会变得与严格的定义一致。

通常，$H(R)$ 的细节是不重要的，就我们的目的而言，知道中值和四等分就足够了。这样，在机器人的预测及其准确性方面，$(\text{中值})\pm (\text{四分位距离})$ 提供了一个好的思路\footnote{译注：有误，应该是区间 $[\text{低四等分},\,\text{高四等分}]$。}。亦即，在提供给机器人的信息上，$r$ 的真值落入这个区间之内的可能性与落在该区间之外是相同的。同样地，$r$ 落入第一个到第五个六等分区间 机器人给出的概率值 $(\frac{5}{6}) - \frac{1}{6} = \frac{2}{3}$（换言之，$2:1$ 的几率）；$r$ 落入第一个十等分到第九个十等分的几率是 $8:2 = 4:1$，以此类推。

对于这些我们在过去经常用到的这些分布，尽管可以发展出相当混乱的近似公式，但是现在很容易通过计算机精确