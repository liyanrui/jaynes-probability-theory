\chapter[quantitative-rules]{初等采样理论}

至此，我们所拥有并且有效的数学基础由基本的乘法和加法规则构成：

\placeformula[3-1]
\startformula
P(AB|C) = P(A|BC)P(B|C) = P(B|AC)P(A|C)
\stopformula

\placeformula[3-2]
\startformula
P(A|B) + P(\itbar{A}|B) = 1
\stopformula

从它们可以导出扩展的加法规则：

\placeformula[3-3]
\startformula
P(A + B|C) = P(A|C) + P(B|C) - P(AB|C)
\stopformula

使用一致性的祈求 (\Roman{3}c)，亦即无差异性原理：如果一组假设 $(H_1,H_2,\cdots,H_N)$ 在背景信息 $B$ 上相互独立且详尽，并且 $B$ 不会偏好这些假设中的任何一个，那么

\placeformula[3-4]
\startformula
P(H_i|B) = \frac{1}{N}\quad\quad 1\le i \le N
\stopformula

从 (\in[3-3]) 和 (\in[3-4]) 又可以推导出 Bernoulli 瓮规则：如果 $B$ 指定了 $A$ 在 $M$ 个假设所构成的子集上为真，而在剩下的 $(N - M)$ 假设所构成的子集为假，那么

\placeformula[3-5]
\startformula
P(A|B) = \frac{M}{N}
\stopformula

意识到概率论在内容上有多少能够仅仅从此式推导出来，这一点非常重要。

实际上，当前所教授的概率论的全部内容，加上许多经常重要结果——它们经常被视为超出了概率论范围，可以从上述基础推导出来。接下来的几章内容会给出一些细节，而后在第 11 章，继续发展我们的机器人的大脑，让它能够充分理解高级应用所需要的另外一些原理。

在本章中，我们的概率论，它的第一个应用与我们之后所期望达到的严肃科学推断相比，相当简单和幼稚。无论如何，我们从细节上来考虑这些，原因并不仅仅是面向教学。对于这些最为简单的应用，对其逻辑的误解，是几十年来阻碍科学推断发展进程——也包含科学本身——的主要原因之一。因而我们强烈建议读者，即使你早已熟悉初等采样理论，在处理更为复杂的问题之前，也应当认真消化本章内容。

\section[sec-3-1]{无放回采样}

为了让 Bernoulli 瓮更为明确，我们定义了以下命题：

\definedescription[mydefinition][location=left, before=, after=, headstyle=\tf, width=broad, distance=0.25em]
\def\myproposition#1{\hbox to 1.5em{#1}$\equiv$}
\mydefinition{\myproposition{$B$}} 有个瓮，里面有 $N$ 个球。它们的编号是 $(1,2,\cdots,N)$，其中有 $M$ 个球是红色的，其余的是白色的，$0\le M \le N$。除此之外，这些球的各方面都相同。闭着眼从瓮中取一个球，观察并记录它的颜色，再把它放回去。重复这一过程 $n$ 次，$0\le n \le N$。\par
\mydefinition{\myproposition{$R_i$}} 第 $i$ 次取的球为红色。\par
\mydefinition{\myproposition{$W_i$}} 第 $i$ 次取的球为白色。\par

根据 $B$，能够取的球仅有红色和白色，有

\placeformula[3-6]
\startformula
P(R_i|B) + P(W_i|B) = 1\quad\quad 1\le i \le N
\stopformula

它等同于说，在知识 $B$ 所创立的\quotation{逻辑环境}里，命题 $R_i$ 与 $W_i$ 的关系是互反的，即

\placeformula[3-7]
\startformula
\itbar{R_i} = W_i\quad\quad \itbar{W_i} = R_i
\stopformula

并且，对于第一次取球，(\in[3-5]) 变为

\placeformula[3-8]
\startformula
P(R_i|B) = \frac{M}{N}
\stopformula

\placeformula[3-9]
\startformula
P(W_i|B) = 1 - \frac{M}{N}
\stopformula

现在来看这意味着什么。概率赋值 (\in[3-8]) 和 (\in[3-9]) 所断言的并非瓮的物理属性或其内容；它们描述的是机器人在取球之前所具备的{\bf 知识状态}。实际上，倘若机器人的知识状态与上面定义的 $B$ 有所不同（例如，它知道红球和白球在瓮内的位置，或者它不知道 $N$ 和 $M$ 多大），那么它为 $R_i$ 和 $W_i$ 所赋的概率必然与 (\in[3-8]) 和 (\in[3-9]) 不同，而瓮的现实属性却不会变。

因此，称用瓮来做实验以\quotation{验证}(\in[3-8])，这种说法，如同在狗身上做实验来验证一个孩子对他的狗的喜爱，毫无逻辑可言。眼下，我们关心的是来自不完备信息的一致性推理所涉及的逻辑，而不是瓮中取出什么东西这一物理现象的断言（任何情况下，着都是不可能做到的，因为信息 $B$ 不完备）。

最终，我们的机器人将会能够作出一些非常肯定的现实预测。这些预测能够接近，但是（除非在一些退化的情况下）并不能真正达到逻辑演绎的确定性；但是，在我们能够说出什么量能够被准确预测并且为此需要何种信息之前，理论还需要进一步发展。换言之，由机器人在不同知识状态下所赋的概率与实验中可观察到的事实，这二者之间的关系可能并非随意建立的；我们有充分的理由仅仅使用可由概率论规则推导出来的那些关系，这是是我们现在正要去做的的事。

当我们向机器人索取与第二次抽取相关的概率时，机器人的知识状态便会出现变化。例如，开始的两次取球，取到的皆为红球的概率是多大？根据乘法规则，这个概率为

\placeformula[3-10]
\startformula
P(R_1R_2|B) = P(R_1|B)P(R_2|R_1B)
\stopformula

在最后一个因子中，机器人必须考虑第一所取的红球已经从瓮中移除，因此瓮中所剩下的 $(N - 1)$ 个球中有 $(M - 1)$ 个是红球。因而

\placeformula[3-11]
\startformula
P(R_1R_2|B) = \frac{M}{N}\frac{M - 1}{N - 1}
\stopformula

沿着这一思路，前 $r$ 次连续取球，所取之球皆为红球的概率为

\placeformula[3-12]
\startformula
\startmathalignment[style=\displaystyle]
\NC P(R_1R_2\cdots R_r|B) \NC = \frac{M(M - 1)\cdots (M - r + 1)}{N(N - 1)\cdots (N - r + 1)}\NR
\NC \NC = \frac{M!(N - r)!}{(M - r)!N!}\quad\quad r\le M\NR
\stopmathalignment
\stopformula

倘若我们用伽马函数关系 $n! = \Gamma(n + 1)$ 来定义阶乘，则限制条件 $r\le M$ 并非必须，因为当 $r > M$ 时，负整数的阶乘结果为无穷大，于是 (\in[3-12]) 的结果自动为 0。

前 $w$ 次取球，取到的球皆为白球的概率与 (\in[3-12]) 相似，只是要将 $M$ 换成 $(N - M)$：

\placeformula[3-13]
\startformula
P(W_1W_2\cdots W_w|B) = \frac{(N - M)!(N - w)!}{(N - M - w)!N!}
\stopformula

假设前 $r$ 次取球，取到的皆为红球，那么在 $(r + 1, r + 2, \cdots,r + w)$ 次取球，取到的皆为白球的概率，需要在 (\in[3-13]) 的基础上考虑 $N$ 和 $M$ 已分别缩减为 $(N - r)$ 和 $(M - r)$：

\placeformula[3-14]
\startformula
P(W_1W_2\cdots W_w|R_1\cdots R_rB) = \frac{(N - M)!(N - r - w)!}{(N - M - w)!(N - r)!}
\stopformula

因此，通过乘法规则，在 $n$ 次取球的过程中，取出 $r$ 个红球之后，又取出 $w = n - r$ 个白球的概率，由 (\in[3-12]) 和 (\in[3-14]) 可得

\placeformula[3-15]
\startformula
P(R_1\cdots R_rW_{r + 1}\cdots W_n|R_1\cdots R_rB) = \frac{M!(N - M)!(N - n)!}{(M - r)!(N - M - w)!N!}
\stopformula

$(N - r)!$ 项被约掉了。

尽管这个结果基于红球和白球特定的取出顺序推导出来，但是以任何特定顺序在 $n$ 次取球中取出 $r$ 个红球，概率都是相同的。为了看清这一点，需要采用下面的方式

\placeformula[3-16]
\startformula
\frac{M!}{(M - r)!} = M(M - 1)\cdots (M - r + 1)
\stopformula

对 (\in[3-15]) 的其他比率作更完全地展开。于是 (\in[3-15]) 的右部变为

\placeformula[3-17]
\startformula
\frac{M(M - 1)\cdots (M - r + 1)(N - M)(N - M - 1)\cdots (N - M - w + 1)}{N(N - 1)\cdots (N - n + 1)}
\stopformula

现在假设 $r$ 个红球和 $(n - r) = w$ 个白球以任意次序被取出。这种情况的概率是 $n$ 个因子的积；每次取到红球，就会存在一个因子 $\frac{(\text{瓮内红球的数量})}{(\text{瓮内球的总数})}$，并且，同样可以为取到白球写出相似的因子。每次取球，瓮内的球的数量便会少一个；因而第 $k$ 次取球，$(N - k + 1)$ 就会在分母中出现，无论在取这个球之前所取的那些球是什么颜色。

就在第 $k$ 个红球被取出之前，无论这个球是在第 $k$ 次还是第 $k$ 次之后的任意一次取球时被取出，瓮内剩下 $(M - k + 1)$ 个红球；因而，取第 $k$ 个球，就相当于在分子上放了一个因子 $(M - k + 1)$。就在第 $k$ 个白球被取出之前，瓮内剩下 $(N - M - k + 1)$ 个红球；因而，取第 $k$ 个球，无论这个球是在第 $k$ 次还是第 $k$ 次之后的任意一次取球时被取出，就相当于在分子上放了一个因子 $(N - M - k + 1)$。因而，此时 $n$ 个球皆已取出，其中 $r$ 个是红球，我们在分子和分母上累积的因子与 (\in[3-17]) 相同；不同的取球次序仅仅是排列分子中因子的次序罢了。因此，在 $n$ 次取球的过程中，以任意次序刚好取出 $r$ 个红球，这种情况的概率皆由 (\in[3-15]) 而定。

务必注意，在这个结果中，乘法规则是以一种特殊的方式展开的，这种方式向我们揭示了如何将计算组织成因子的乘积，{\bf 给定之前的所有取球的结果}，每一个因子都是一次特定的取球结果的概率。但是还有许多其他方式可以将这个乘法规则展开，每一种展开结果都能够给出以不止所有之前的取球结果为条件的因子；所有的这些计算一定会得到相同的结果，这是一个事实，一个非常重要的一致性性质，是第 \in[quantitative-rules] 章的推导所要致力确定的。

接下来，我们问：在 $n$ 次取球的过程中，无关乎次序，恰好取到 $r$ 个红球的概率是多大？红球与白球出现不同的次序的可能性相互独立，因此我们要把这些可能性叠加起来；由于每一项都等于 (\in[3-15])，所以只需要用二项式系数

\placeformula[3-18]
\startformula
\startpmatrix n\NR r\NR\stoppmatrix = \frac{n!}{r!(n - r)!}
\stopformula

乘以 (\in[3-15])。二项式系数表示 $n$ 次取球恰好取到 $r$ 个红球的可能取法的数量，我们将其称为事件 $r$ 的{\bf 多样性}。例如，在三次取球过程中取到三个红球，只有一种取法，因为

\placeformula[3-19]
\startformula
\startpmatrix 3\NR 3\NR\stoppmatrix = 1
\stopformula

这种取法是 $R_1R_2R_3$，亦即事件 $r = 3$ 的多样性为 1。但是，要从三次取球过程中获得两个红球，取法有三种，因为

\placeformula[3-20]
\startformula
\startpmatrix 3\NR 2\NR\stoppmatrix = 3
\stopformula

这三种取法分别是 $R_1R_2W_3$、$R_1W_2R_3$ 以及 $W_1R_2R_3$，因此事件 $r = 2$ 的多样性为 $3$。

\startExercise
为什么 (\in[3-18]) 的多样性因子不是 $n!$？毕竟，我们是从规定小球不仅有颜色而且也有标签 $(1,2,\cdots,N)$，因此红球本身的不同排列——为 (\in[3-18]) 的分母贡献了 $r!$，造成有区别的 $n$ 次取球的结果。

提示：在 (\in[3-15]) 中，我们没有指定哪些红球和哪些白球要被抽取。
\stopExercise

计算 (\in[3-15]) 和 (\in[3-18]) 的积，许多阶乘可以重新组织成三个二项式系数。定义 $A\equiv$\quotation{以任意次序取 $n$ 个球，其中有 $r$ 个红球}，还有函数

\placeformula[3-21]
\startformula
h(r|N, M, n) \equiv P(A|B)
\stopformula

这样就有

\placeformula[3-22]
\startformula
h(r|N, M, n) = \frac{\startpmatrix M\NR r\NR\stoppmatrix\startpmatrix N - M\NR n - r\NR\stoppmatrix}{\startpmatrix N\NR n\NR\stoppmatrix}
\stopformula

我们应当将其缩写为 $h(r)$。通过约定 $x! = \Tau(x + 1)$，当 $r > M$ 或 $r > n$ 或者 $(n - r) > (N - M)$ 时，$h(r)$ 就会自动变为 $0$，理应如此。

在此，我们动用了记法上的小伎俩，原因在附录 B 部分给出了解释。要点是，正式的概率符号 $P(A|B)$，用的是大写字母 $P$，参数 $A$ 和 $B$ 总是表示命题，它们可以是相当复杂的语言陈述。若想用普通数字作为参数，那么为了一致性，我们应当定义新的函数符号，诸如 $h(r|N, M, n)$。坚持使用 $P(r|NMn)$，忽视了 $A$ 和 $B$ 定性方面的约定，会因为曲解了这些方程的意思而导致严重的错误（例如以后要讨论的一些生僻的悖论）。不过，如第 \in[quantitative-rules] 章所述，我们通过使用 $p(A|B)$ 或 $p(r|n)$ 这样的使用小写字母 $p$ 的概率符号来迎合现代的工作，在这些符号中，我们允许使用命题或代数变量作为参数；在这种情况下，它们的含义由语境而定。

基本结果 (\in[3-22]) 被称为\quotation{超几何分布}，因为它与高斯超几何函数的幂级数形式

\placeformula[3-23]
\startformula
F(a,b,c;t) = \sum_{r = 0}^{\infty}\frac{\Tau(a + r)\Tau(b + r)\Tau(c)}{\Tau(a)\Tau(b)\Tau(c + r)}\frac{t^r}{r!}
\stopformula

的系数有关。如果 $a$ 和 $b$ 中有一个为负整数，这个级数会收敛，因此它是个多项式。很容易验证，{\bf 生成函数}

\placeformula[3-24]
\startformula
G(t) \equiv \sum_{r = 0}^nh(r|N, M, n)t^r
\stopformula

等于

\placeformula[3-25]
\startformula
G(t) = \frac{F(-M,-n,c;t)}{F(-M,-n,c;1)}
\stopformula

其中 $c = N - M - n + 1$。显然 $G(1) = 1$，根据 (\in[3-24])，这个等式描述的是超几何分布被正确地归一化了。在 (\in[3-25]) 的结果中，$G(t)$ 满足二阶超几何微分方程，并且有一些与计算有用的其他性质。

尽管超几何分布 $h(r)$ 看上去很复杂，但是它有着一些令人惊讶的简单性质。通过设定 $h(r') = h(r' - 1)$，$r$ 最有可能的值被发现位于一个单位之内。我们发现的是

\placeformula[3-26]
\startformula
r' = \frac{(n + 1)(M + 1)}{N + 2}
\stopformula

如果 $r'$ 是整数，那么 $r'$ 和 $r' - 1$ 都是最有可能的值。如果 $r'$ 并非整数，那么就会有唯一的最可能的值是

\placeformula[3-27]
\startformula
\hat{r}=\text{INT}(r')
\stopformula

即比 $r'$ 小的整型值。因而采样取球中红球的最有可能的分数 $f = \frac{r}{m}$，如同我们的直觉所期盼的那样，它近似等于分数 $F = \frac{M}{N}$，后者是瓮内原有的红球的个数与球的总数的比值。在现实预测方面，这是我们的第一个粗略的示例：我们的信息所指定的量 $F$ 与现实实验所测定的量 $f$ 之间的关系，而这个关系是根据概率论推导出来的。

超几何分布 $h(r)$ 的宽度给出了精确性指标，利用这个指标，机器人可以判定 $r$。许多这样的问题，它们的答案可通过计算{\bf 累加概率}而得到，即寻找 $R$ 个或更少的红球的概率。如果 $R$ 是整数，累加概率就是

\placeformula[3-28]
\startformula
H(R) \equiv\sum_{r = 0}^Rh(r)
\stopformula

不过，为了后文中给出正式的原因，我们将 $H(x)$ 定义成一种阶梯函数，$x$ 值为所有非负实数；因而 $H(x)=H(R)$，其中 $R = \text{INT}(x)$ 是不大于 $x$ 的最大整数。

对于诸如 $h(r)$ 这样的概率分布，将其{\bf 中值}定义为一个数 $m$，命题 $r < m$ 和 $r > m$ 被赋予相等的概率。严格而言，根据这个定义，一个离散的分布通常没有中值。倘若存在一个整数 $R$，满足 $H(R - 1) = 1 - H(R)$，并且 $H(R) > H(R - 1)$，则 $R$ 是唯一的中值。倘若有一个整数 $R$，满足 $H(R) = \frac{1}{2}$，则 $R\le r < R'$ 之内的任意 $r$ 都是中值，其中 $R'$ 是 $H(x)$ 的下一个比较高的跃点；否则不存在中值。

但是对于大多数目的而言，我们可能要宽容一些，以接近定义为准。如果 $n$ 是足够大，那么便有理由认为能够让 $H(R)$ 非常接近 $\frac{1}{2}$ 的 $R$ 为\quotation{中值}。以相同的宽容精神，使得 $H(R)$ 非常接近 $\frac{1}{4}$、$\frac{3}{4}$ 的 $R$ 值，可将其称为\quotation{低四等分}和\quotation{高四等分}。如果 $n \gg 10$，使得 $H(R)$ 非常接近 $\frac{k}{10}$ 的 $R$ 值，我们称为\quotation{第 $k$ 个十等分}，以此类推。当 $n \rightarrow \infty$，这些宽松的定义就会变得与严格的定义一致。

通常，$H(R)$ 的细节是不重要的，就我们的目的而言，知道中值和四等分就足够了。这样，在机器人的预测及其准确性方面，$(\text{中值})\pm (\text{四分位距离})$ 提供了一个好的思路\footnote{译注：有误，应该是区间 $[\text{低四等分},\,\text{高四等分}]$。}。亦即，在提供给机器人的信息上，$r$ 的真值落入这个区间之内的可能性与落在该区间之外是相同的。同样地，$r$ 落入第一个到第五个六等分区间 机器人给出的概率值 $(\frac{5}{6}) - \frac{1}{6} = \frac{2}{3}$（换言之，$2:1$ 的几率）；$r$ 落入第一个十等分到第九个十等分的几率是 $8:2 = 4:1$，以此类推。

对于这些我们在过去经常用到的这些分布，尽管可以发展出相当混乱的近似公式，但是现在很容易通过计算机算出精确的分布。例如，W. H. Press 等 (1986) 给出了两个程序，在 $a$、$b$ 和 $c$ 值任意的情况下，它们可以算出广义复超几何分布。表 \in[t-3-1] 与 \in[t-3-2] 分别给出了 $N = 100$、$M = 50$、$n = 10$ 和 $N = 100$、$M = 10$、$n = 50$ 这两种情况下的超几何分布。在后者中，不可能取出多于 $10$ 个球，因此 $r > 10$ 的条目都是 $h(r) = 0$，$H(r) = 1$，因此在表中对它们未作统计。惊人的事实是，$h(r)$ 正值对应的条目是相同的，亦即超几何分布在交换 $M$ 和 $n$ 的情况下具有对称性质

\placeformula[3-29]
\startformula
h(r|N, M, n) = h(r|N, n, M)
\stopformula

无论我们是从包含 50 个红球的瓮中取十个球，还是从包含十个红球的瓮中取 50 个球，在样本抽取过程中取到 $r$ 个红球的概率是相同的。这个性质很容易通过详细检验 (\in[3-22]) 而得以验证；从超几何函数 (\in[3-23]) 的 $a$ 与 $b$ 的对称性来看，这一点也显而易见。

表 \in[t-3-1] 与 \in[t-3-2] 还有着另外一个显而易见的对称性，关于分布尖峰的对称性：$h(r|100, 50， 10) = h(10 - r|100, 50, 10)$。不过，通常情况下并非如此。将 $N$ 的值改为 $99$，就会导致轻微地非对称尖峰，如表 \in[t-3-3] 所示。表 \in[t-3-1] 的尖峰的出现，原因在于：如果交换 $M$ 和 $(N - M)$，并且同时交换 $r$ 和 $(n - r)$，那么就相当于\quotation{红球}和\quotation{白球}这两个单词的交换，因此分布未变：

\placeformula[3-30]
\startformula
h(n - r|N, N - M, n) = h(r|N, M, n)
\stopformula

但是当 $M = \frac{N}{2}$，上式可约化为对称形式

\placeformula[3-31]
\startformula
h(n - r|N, M, n) = h(r|N, M, n)
\stopformula

这正是在表 \in[t-3-1] 中我们所看到的尖峰的对称性。根据 (\in[3-29])，当 $n = \frac{N}{2}$ 时，尖峰也是对称的。

\placetable[here,force][t-3-1]{超几何分布：$N,M,n=100,10,50$}
{\startxtable[offset=0pt]
  \setupinterlinespace[line=1.2em]
  \startxtablehead[topframe=on, rulethickness=1.5pt]
  \startxrow
  \startxcell[width=3cm] $r$ \stopxcell
  \startxcell[width=3cm] $h(r)$ \stopxcell
  \startxcell[width=3cm] $H(r)$ \stopxcell
  \stopxrow
  \stopxtablehead
  \startxtablebody[height=fit]
  \startxrow[topframe=on,rulethickness=0.75pt]
  \startxcell 0 \stopxcell
  \startxcell 0.000593  \stopxcell
  \startxcell 0.000593 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 1 \stopxcell
  \startxcell 0.007237 \stopxcell
  \startxcell 0.007830 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 2 \stopxcell
  \startxcell 0.037993 \stopxcell
  \startxcell 0.045824 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 3 \stopxcell
  \startxcell 0.113096 \stopxcell
  \startxcell 0.158920 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 4 \stopxcell
  \startxcell 0.211413 \stopxcell
  \startxcell 0.370333 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 5 \stopxcell
  \startxcell 0.259334 \stopxcell
  \startxcell 0.629667 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 6 \stopxcell
  \startxcell 0.211413 \stopxcell
  \startxcell 0.841080 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 7 \stopxcell
  \startxcell 0.113096 \stopxcell
  \startxcell 0.954177 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 8 \stopxcell
  \startxcell 0.037993 \stopxcell
  \startxcell 0.992170 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 9 \stopxcell
  \startxcell 0.007237 \stopxcell
  \startxcell 0.999407 \stopxcell
  \stopxrow
  \stopxtablebody
  \startxtablefoot[bottomframe=on,rulethickness=1.5pt]
  \startxrow
  \startxcell 10 \stopxcell
  \startxcell 0.000593 \stopxcell
  \startxcell 1.000000 \stopxcell
  \stopxrow
  \stopxtablefoot
  \stopxtable
}

\placetable[here,force][t-3-2]{超几何分布：$N,M,n=100,50,10$}
{\startxtable[offset=0pt]
  \setupinterlinespace[line=1.2em]
  \startxtablehead[topframe=on, rulethickness=1.5pt]
  \startxrow
  \startxcell[width=3cm] $r$ \stopxcell
  \startxcell[width=3cm] $h(r)$ \stopxcell
  \startxcell[width=3cm] $H(r)$ \stopxcell
  \stopxrow
  \stopxtablehead
  \startxtablebody[height=fit]
  \startxrow[topframe=on,rulethickness=0.75pt]
  \startxcell 0 \stopxcell
  \startxcell 0.000593  \stopxcell
  \startxcell 0.000593 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 1 \stopxcell
  \startxcell 0.007237 \stopxcell
  \startxcell 0.007830 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 2 \stopxcell
  \startxcell 0.037993 \stopxcell
  \startxcell 0.045824 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 3 \stopxcell
  \startxcell 0.113096 \stopxcell
  \startxcell 0.158920 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 4 \stopxcell
  \startxcell 0.211413 \stopxcell
  \startxcell 0.370333 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 5 \stopxcell
  \startxcell 0.259334 \stopxcell
  \startxcell 0.629667 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 6 \stopxcell
  \startxcell 0.211413 \stopxcell
  \startxcell 0.841080 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 7 \stopxcell
  \startxcell 0.113096 \stopxcell
  \startxcell 0.954177 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 8 \stopxcell
  \startxcell 0.037993 \stopxcell
  \startxcell 0.992170 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 9 \stopxcell
  \startxcell 0.007237 \stopxcell
  \startxcell 0.999407 \stopxcell
  \stopxrow
  \stopxtablebody
  \startxtablefoot[bottomframe=on,rulethickness=1.5pt]
  \startxrow
  \startxcell 10 \stopxcell
  \startxcell 0.000593 \stopxcell
  \startxcell 1.000000 \stopxcell
  \stopxrow
  \stopxtablefoot
  \stopxtable
}

\placetable[here,force][t-3-3]{超几何分布：$N,M,n=100,50,10$}
{\startxtable[offset=0pt]
  \setupinterlinespace[line=1.2em]
  \startxtablehead[topframe=on, rulethickness=1.5pt]
  \startxrow
  \startxcell[width=3cm] $r$ \stopxcell
  \startxcell[width=3cm] $h(r)$ \stopxcell
  \startxcell[width=3cm] $H(r)$ \stopxcell
  \stopxrow
  \stopxtablehead
  \startxtablebody[height=fit]
  \startxrow[topframe=on,rulethickness=0.75pt]
  \startxcell 0 \stopxcell
  \startxcell 0.000527 \stopxcell
  \startxcell 0.000527 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 1 \stopxcell
  \startxcell 0.006594 \stopxcell
  \startxcell 0.007121 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 2 \stopxcell
  \startxcell 0.035460 \stopxcell
  \startxcell 0.042581 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 3 \stopxcell
  \startxcell 0.108070 \stopxcell
  \startxcell 0.150651 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 4 \stopxcell
  \startxcell 0.206715 \stopxcell
  \startxcell 0.357367 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 5 \stopxcell
  \startxcell 0.259334 \stopxcell
  \startxcell 0.616700 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 6 \stopxcell
  \startxcell 0.216111 \stopxcell
  \startxcell 0.832812 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 7 \stopxcell
  \startxcell 0.118123 \stopxcell
  \startxcell 0.950934 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 8 \stopxcell
  \startxcell 0.040526 \stopxcell
  \startxcell 0.991461 \stopxcell
  \stopxrow
  \startxrow
  \startxcell 9 \stopxcell
  \startxcell 0.007880 \stopxcell
  \startxcell 0.999341 \stopxcell
  \stopxrow
  \stopxtablebody
  \startxtablefoot[bottomframe=on,rulethickness=1.5pt]
  \startxrow
  \startxcell 10 \stopxcell
  \startxcell 0.000659 \stopxcell
  \startxcell 1.000000 \stopxcell
  \stopxrow
  \stopxtablefoot
  \stopxtable
}

超几何分布还有两个不直观甚至在 (\in[3-22]) 中也难以发现的对称性质。我们问机器人索求第二次取到红球的概率 $P(R_2|B)$。这与 (\in[3-8]) 的计算并不相同，因为机器人知道，在进行第二次取球之前，瓮内只有 $(N - 1)$ 个球，而非 $N$ 个。但是，机器人不知道第一次取球而导致瓮内少了的那个球的颜色，因此它不知道瓮内剩下的红球的数量是 $M$ 还是 $(M - 1)$。这样一来，Bernoulli 瓮的的根基 (\in[3-5]) 就丢了，从而导致问题似乎变得不可判定起来。

然而，这个问题终究是可判定的；下面便是概率计算中有用的技巧的第一个示例，其原理是将一个命题分解为几个更为简单的命题的析取，这个原理在第 1 章和第 2 章讨论过了。机器人知道 $R_1$ 和 $W_1$ 必有一个为真，因而基于布尔代数，我们就有

\placeformula[3-32]
\startformula
R_2 = (R_1 + W_1)R_2 = R_1R_2 + W_1R_2
\stopformula

应用加法规则和乘法规则，可得

\placeformula[3-33]
\startformula
\startmathalignment[style=\displaystyle]
\NC P(R_2|B)\NC = P(R_1R_2|B) + P(W_1R_2|B)\NR
\NC \NC = P(R_2|R_1B)P(R_1|B) + P(R_2|W_1B)P(W_1|B)\NR
\stopmathalignment
\stopformula

然而

\placeformula[3-34]
\startformula
P(R_2|R_1B) = \frac{M - 1}{N - 1}\quad\quad P(R_2|W_1B) = \frac{M}{N - 1}
\stopformula

因此

\placeformula[3-35]
\startformula
P(R_2|B) = \frac{M - 1}{N - 1}\frac{M}{N} + \frac{M}{N - 1}\frac{N - M}{N} = \frac{M}{N}
\stopformula

复杂性顷刻之间便消失了，而且第一次和第二次取到红球的概率是相同的。下面继续看第三次会发生什么

\placeformula[3-36]
\startformula
R_3 = (R_1 + W_1)(R_2 + W_2)R_3 = R_1R_2R_3 + R_1W_2R_3 + W_1R_2R_3 + W_1W_2R_3
\stopformula

因此

\placeformula[3-37]
\startformula
\startmathalignment[style=\displaystyle]
\NC P(R_3|B)\NC = \frac{M}{N}\frac{M - 1}{N - 1}\frac{M - 2}{N - 2} + \frac{M}{N}\frac{N - M}{N - 1}\frac{M - 1}{N - 2}\NR
\NC \NC +  \frac{N - M}{N}\frac{M}{N - 1}\frac{M - 1}{N - 2} + \frac{N - M}{N}\frac{N - M - 1}{N - 1}\frac{M}{N - 2}\NR
\NC \NC = \frac{M}{N}
\stopmathalignment
\stopformula

复杂性又一次被我们消除了。对于任意一次取到红球，机器人{\bf 如果不知道任何其他次取球的结果}，它总会赋以与 Bernoulli 瓮的结果 (\in[3-5]) 相同的概率。这就是第一个不明显的对称性。在此，我们不对这个结论的一般性给出证明，因为这个结论是一个更具一般性的结果的特殊情况；见后文的方程 (\in[3-118])。

(\in[3-32]) 和 (\in[3-26]) 所展示的计算方法总结如下：对于需要赋以概率的量，将其分解为相互独立的子命题，然后运用加法规则和乘法规则。若子命题经过合理挑选（亦即，如果他们在问题的语境中有着简单的含义），那么它们的概率通常易于计算。若挑选不当（如同第 2 章末尾所给出的企鹅的例子），则与之相关的计算过程自然是毫无帮助。

\section{逻辑 vs 偏好}

\in[sec-3-1] 节的结果给我们带来了新的问题。在索求第 $k$ 次取球结果为红球的概率时，在之前的取球过程中所取到了何种颜色的球，这一知识显然与我们要解决的问题有关系，因为之前的取球结果会影响在第 $k$ 次取球时瓮内红球的数量 $M_k$。那么之后取球结果会不会也对第 $k$ 次有影响呢？第一感觉是没有影响，因为之后的取球结果不会影响到 $M_k$ 的值。例如，广为人知的统计力学的阐述（Penrose，1979）依赖一个基本公理，现在发生的事情仅依赖之前发生的事，并不依赖之后发生的事。Penrose 认为这是\quotation{因果}所必须的客观条件。

因而，如同在第 1 章所作的那样，我们需要再一次强调，推断关心的是{\bf 逻辑}联系，它可能与具有赢过关系的物理影响有关，也可能无关。为了表明为什么之后发生的事件的知识与之前发生的事件的概率存在联系，不妨考虑一个已知（背景信息 $B$）仅包含一个红球和一个白球的瓮：$N = 2$，$M = 1$。仅给定这个信息，第一次取到红球的概率为 $P(R_1|B) = \frac{1}{2}$。但是，倘若机器人知道第二次抽到了红球，那么第一次便不可能取到红球，即

\placeformula[3-38]
\startformula
P(R_1|R_2B) = 0
\stopformula

更为一般的是，乘法规则给出

\placeformula[3-39]
\startformula
P(R_jR_k|B) = P(R_j|R_kB)P(R_k|B) = P(R_k|R_jB)P(R_j|B)
\stopformula

但是我们已经看到了，对于所有的 $j$、$k$，$P(R_j|B) = P(R_k|B) = \frac{M}{N}$，因此

\placeformula[3-40]
\startformula
P(R_j|R_kB) = P(R_k|R_jB)
\stopformula

概率论告诉我们的是，后续的取球结果与第 $k$ 次取球结果的相关性与之前的取球结果完全相同！尽管完成后续的某次取球并不会对第 $k$ 次取球时瓮内的红球数量 $M_k$ 产生物理影响，但是与下一次取球有关的{\bf 信息}与之前的某次取球相比，在我们的在第 $k$ 次取球时的{\bf 知识状态}上有着同样的影响。这就是第二个不明显的对称性。

这个结果可能会让一些关心\quotation{概率的含义}的一些派系非常不安。尽管逻辑蕴含与物理因果并不相同这一点广为人知，但是尝试将概率 $P(A|B)$ 解释为 $B$ 对 $A$ 的某种程度上的因果影响，这种倾向依然非常顽固。不仅上述的 Penrose 的工作体现了这种倾向，哲学家 Karl Popper\footnote{在第九次 Colston 研讨会上，Popper (1957) 将他的偏好解释描述为\quotation{纯客观的}，只是回避了\quotation{物理影响}这种描述。实际上，他会认为，抛掷一个骰子，出现哪一面的概率并非骰子的物理形式（如同 Cram\'er (1946) 所坚持的那样），而是整个实验装置的客观性质。}将概率解释为\quotation{偏好}则更强烈地体现了这一倾向。